{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb5d5f3",
   "metadata": {},
   "source": [
    "# Math 345 Midterm 2 Preparation\n",
    "## Dr. Wicke\n",
    "### Christopher La Valle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72337b05",
   "metadata": {},
   "source": [
    "# Order Statistics\n",
    "\n",
    "**Definition:** Let $X_1,X_2,\\ldots,X_n$ be a sample of $n$ i.i.d. random variables from a distribution with cumulative distribution $F(X)$. The order statistics of this sample are the sample values arranged in non-decreasing order, such that\n",
    "\n",
    "$$X_{(1)}\\le X_{(2)}\\le\\ldots\\le X_{(n)}$$\n",
    "\n",
    "where $X_{(1)}$ represents the minimum of the random sample, $X_{(n)}$ respresents the maximum, and $X_{(r)}$ is called the $r\\text{th}$ order statistic.\n",
    "\n",
    "**Joint Density:** The random process itself is simply drawing $n$ i.i.d., unordered samples. The order statistics are defined after the fact by sorting them. There are $n!$ different ways to draw $n$ different values, but each permutation has probabilty $\\prod_{i=1}^nf(x_i)$. Therefore, the joint density function of the order statistics is the product of the $n!$ different ways to draw $n$ values.\n",
    "\n",
    "$$f(x_{(1)}, x_{(2)},\\ldots,x_{(n)})=n!\\prod_{i=1}^nf(x_{i})$$\n",
    "\n",
    "\n",
    "**Support:**\n",
    "\n",
    "**Marginal Density:** \n",
    "\n",
    "$$f_{X_{(r)}}(x)=\\frac{n!}{(n-r)!(r-1)!}[F(x)]^{r-1}[1-F(x)]^{n-r}f(x)$$\n",
    "\n",
    "**Bi-Density:**\n",
    "\n",
    "$$f_{X_{(i)},X_{(j)}}(x_i,x_j)=\\frac{n!}{(i-1)!(j-i-1)!(n-j)!}[F(x_i)]^{i-1}f(x_i)[F(x_j)-F(x_i)]^{j-i-1}f(x_j)[1-F(x_j)]^{n-j}$$\n",
    "\n",
    "**Cumulative Distribution Function:** Notice that the $j\\text{th}$ order statistic is less than or equal to $y$ if and only if there are $j$ or more of the $X_i$'s that are less than or equal to $y$. Thus, because the number of $X_i$'s that are less than or equal to $y$ is a binomical reandom variabel with parameters $n,p=F(y)$, it follows that\n",
    "\n",
    "$$\\begin{align*}\n",
    "F_{X_{(j)}}&=P(X_{(j)}\\le y)\\\\\n",
    "&=P(j\\text{ or more of the }X_i\\text{'s are }\\le y)\\\\\n",
    "&=\\sum_{k=j}^n\\begin{pmatrix}n\\\\k\\end{pamtrix}[F(y)]^k[1-F(y)]^{n-k}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e9083",
   "metadata": {},
   "source": [
    "# Jacobian Matrix and Determinant\n",
    "\n",
    "\n",
    "**Definition:** Suppose $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ is such that each of its first-order partial derivatives exists on $\\mathbb{R}^n$, then the Jacobian matrix of $f$ is defined as\n",
    "\n",
    "**Application to Probability:** Suppose you are given $n$ random variable $X_1,X_2,\\ldots,X_n$ and\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = g(\\mathbf{X}) = \n",
    "\\begin{pmatrix}\n",
    "g_1(X_1,\\dots,X_n) \\\\\n",
    "\\vdots \\\\\n",
    "g_n(X_1,\\dots,X_n)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{Y}}(\\mathbf{y}) \n",
    "= f_{\\mathbf{X}}\\bigl(g^{-1}(\\mathbf{y})\\bigr)\\,\\bigl|\\det J_{g^{-1}}(\\mathbf{y})\\bigr|,\n",
    "$$\n",
    "\n",
    "assuming the functions $g_i$ have continuous partial derivatives, the Jacobian determinant is not equal to zero, and that the equations $y_i$ have a unique solution, say $x_i=g^{-1}(y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512cbeb",
   "metadata": {},
   "source": [
    "# Exchangeable Random Variables\n",
    "\n",
    "**Definition:**  The random variables $X_1,X_2,\\ldots,X_n$ are said to be exchangeable if, for every permutation $i_1,\\ldots,i_n$ of the integers $1,\\ldots,n$,\n",
    "\n",
    "$$P(X_{i_1}\\le x_1,X_{i_2}\\le x_2,\\ldots,X_{i_n}\\le x_n)=P(X_1\\le x_1,X_2\\le x_2,\\ldots,X_n\\le x_n)$$\n",
    "\n",
    "This means that the PDF or CDF of ($X_1,\\ldots,X_n$) must be the same as that for $(X_{i_1},\\ldots,X_{i_n})$.\n",
    "\n",
    "**Discrete Case:** $P(X_{i_1}=x_1,X_{i_2}=x_2,\\ldots,X_{i_n}=x_n)=P(X_1=x_1,X_2=x_2,\\ldots,X_n=x_n)$. In particular, the joint PMF $p(x_1,\\ldots,x_n)$ is a symmetric function of the vector $(x_1,\\ldots,x_n)$, which means that its value does not chagne when the values of the vector are permuted.\n",
    "\n",
    "**Independence:** If $X_1,\\ldots,X_n$ are i.i.d., then they are exchangeable\n",
    "\n",
    "# Proof?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca719c5",
   "metadata": {},
   "source": [
    "# Expectation Random Variables\n",
    "\n",
    "**Definition:** If $X$ and $Y$ have a joint pmf $p(x,y)$, then\n",
    "\n",
    "$$E[g(X,Y)]=\\sum_y\\sum_xg(x,y)\\cdot p(x,y)$$\n",
    "\n",
    "If $X$ and $Y$ have a joint pdf $f(x,y)$, then\n",
    "\n",
    "$$E[g(X,Y)]=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}g(x,y)\\cdot f(x,y)dxdy$$\n",
    "\n",
    "**Expectation of the Sum of Two Random Variables:** If $E(X)$ and $E(Y)$ are finite and $g(X,Y)=X+Y$, then\n",
    "\n",
    "$$\\begin{align*}\n",
    "E[X+Y]&=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x+y)f(x,y)dxdy\\\\\n",
    "&=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x\\cdot f(x,y)dydx+\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}y\\cdot f(x,y)dxdy\\\\\n",
    "&=\\int_{-\\infty}^{\\infty}x\\cdot f(x)dx+\\int_{-\\infty}^{\\infty}y\\cdot f(y)dy\\\\\n",
    "&=E(X)+E(Y)\n",
    "\\end{align*}$$\n",
    "\n",
    "Moreover, \n",
    "\n",
    "$$E(X-Y)=E(X)-E(Y)$$\n",
    "\n",
    "Additionally, if $X\\ge Y$, \n",
    "\n",
    "$$E(X)\\ge E(Y)$$\n",
    "\n",
    "Using induction, one can show that if $E(X_i)$ is finite for all $i=1,\\ldots,n$, then\n",
    "\n",
    "$$E(X_1+\\ldots+X_n)=E(X_1)+\\ldots+E(X_n)$$\n",
    "\n",
    "**Boole's Inequality:** Let $A_1,\\ldots,A_n$ denote events. Then, Boole's inequality says\n",
    "\n",
    "$$P(\\cup_{i=1}^nA_i)\\le\\sum_{i=1}^nP(A_i)$$\n",
    "\n",
    "Word of caution at the end of lecture 14\n",
    "\n",
    "**Additional Properties:**\n",
    "\n",
    "$$P(a\\le X\\le b)=1\\implies a\\le E(X)\\le b$$\n",
    "\n",
    "If $X$ is a continuous nonnegative random variable, then\n",
    "\n",
    "$$E(X)=\\int_0^{\\infty}P(X>x)dx$$\n",
    "\n",
    "If $X$ is a discrete nonnegative random variable with values in $\\{0,1,2,\\ldots\\}$, then\n",
    "\n",
    "$$E(X)=\\sum_{n=0}^{\\infty}P(X>n)$$\n",
    "\n",
    "**Expectation of a Product of Independent Random Variables:** If $X$ and $Y$ are independent, then for any functions $h$ and $g$,\n",
    "\n",
    "$$E[g(X)h(Y)]=E[g(X)]E[g(Y)]$$\n",
    "\n",
    "*Proof.*\n",
    "\n",
    "$$E[g(X)g(Y)]=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}g(x)h(y)f(x,y)dxdy$$\n",
    "\n",
    "$$=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}g(x)h(y)\\cdot f_X(x)\\cdot f_Y(y)dxdy$$\n",
    "\n",
    "$$=\\int_{-\\infty}^{\\infty}h(y)f_Y(y)dy\\int_{-\\infty}^{\\infty}g(x)f_X(x)dx$$\n",
    "\n",
    "$$=E[h(y)]E[g(x)]$$\n",
    "\n",
    "**Covariance:** Indicates the strength of the linear relationship between two random variables. The Covariance between $X$ and $Y$ is defined by\n",
    "\n",
    "$$Cov(X,Y)=E[(X-E(X))](Y-E(Y))$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "Cov(X,Y)&=E\\left[XY-E(X)Y-XE(Y)+E(Y)E(X)\\right]\\\\\n",
    "&=E(XY)-E(X)E(Y)-E(X)E(Y)+E(X)E(Y)\\\\\n",
    "&=E(XY)-E(X)E(Y)\n",
    "\\end{align*}$$\n",
    "\n",
    "**Properties of Covariance:** \n",
    "\n",
    " - $Cov(X,Y)=Cov(Y,X)$\n",
    " - $Cov(X,X)=Var(X)$\n",
    " - $Cov(aX,Y)=a Cov(X,Y)$\n",
    " - $Cov(X+Y, Z)=Cov(X,Z)+Cov(Y,Z)$, and more generally\n",
    "\n",
    "$$Cov\\left[\\sum_{i=1}^na_iX_i,\\sum_{j=1}^mb_jY_j\\right]=\\sum_{i=1}^n\\sum_{j=1}^ma_ib_jCov(X_i,Y_j)$$\n",
    "\n",
    "**Variance of Sums:** \n",
    "\n",
    "$$Var\\left[\\sum_{i=1}^nX_i\\right]=\\sum_{i=1}^nVar(X_i)+2\\sum_{1\\le i<j\\le n}Cov(X_i,X_j)$$\n",
    "\n",
    "If $X_1,\\ldots,X_n$ are pairwise independent, then \n",
    "\n",
    "$$Var\\left[\\sum_{i=1}^nX_i\\right]=\\sum_{i=1}^nVar(X_i)$$\n",
    "\n",
    "**Correlation:** The correlation of two random variables $X$ and $Y$, denoted by $\\rho(X,Y)$, is defined, as long as $Var(X)Var(Y)$ is positive, by\n",
    "\n",
    "$$\\rho(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}=\\frac{Cov(X,Y)}{SD(X)SD(Y)}$$\n",
    "\n",
    "**Properties of Correlation:**\n",
    "\n",
    " - $-1\\le\\rho(X,Y)\\le1$\n",
    " - $\\rho(X,Y)=\\rho(Y,X)$\n",
    " - If $Y=a+bX$, then\n",
    "    - if $b>0$, $\\rho(X,Y)=1$\n",
    "    - if $b<0$, $\\rho(X,Y)=-1$\n",
    " - The converse is true: If $\\rho(X,Y)=\\pm1$, then there exists constants $a,b$ with the sign of $b$ equal to $\\rho(X,Y)$, such that $Y=a+bX$ with probabilty one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ac3dd",
   "metadata": {},
   "source": [
    "# Moments of Events\n",
    "\n",
    "**Definition:** For given events $A_1,\\ldots,A_n$, suppose we are interested in the number of pairs of events that occur\n",
    "\n",
    "$$X_iX_j=\\begin{cases}1&\\text{if }A_i\\text{ and }A_j\\text{ occur}\\\\0&\\text{else}\\end{cases}$$\n",
    "\n",
    "Thus, the number of pairs that occur is equal to \n",
    "\n",
    "$$\\sum_{i<j}X_iX_j$$\n",
    "\n",
    "Because $X$ is the number of events that occur, it follows that the number of pairs of events that occur is $\\begin{pmatrix}X\\\\2\\end{pmatrix}$. Consequently,\n",
    "\n",
    "$$\\begin{pmatrix}X\\\\2\\end{pmatrix}=\\sum_{i<j}X_iX_j$$\n",
    "\n",
    "where there are $\\begin{pmatrix}n\\\\2\\end{pmatrix}$$\n",
    "\n",
    "Taking the expectation\n",
    "\n",
    "$$E(\\begin{pmatrix}X\\\\2\\end{pmatrix})=\\sum_{i<j}E[X_iX_j]=\\sum_{i<j}p(A_iA_j)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$E\\left[\\frac{X(X-1)}{2}\\right]=\\sum_{i<j}P(A_iA_j)$$\n",
    "\n",
    "giving that \n",
    "\n",
    "$$E(X^2)-E(X)=2\\cdot \\sum_{i<j}P(A_iA_j)$$\n",
    "\n",
    "Moreover, by considering the number of distinct subsets of $k$ event that all occur, we see that\n",
    "\n",
    "$$\\begin{pmatrix}X\\\\k\\end{pmatrix}=\\sum_{i_1<i_2<\\ldots<i_k}X_{i_1}X_{i_2}\\ldots X_{i_k}$$\n",
    "\n",
    "Taking expectations gives the identity\n",
    "\n",
    "$$E\\left[\\begin{pmatrix}X\\\\k\\end{pmatrix}\\right]=\\sum_{i_1<i_2<\\ldots<i_k}E[X_{i_1}X_{i_2}\\ldots X_{i_k}]=\\sum_{i_1<i_2<\\ldots<i_k}P(A_{i_1}A_{i_2}\\ldots A_{i_k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b01566",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e92b4",
   "metadata": {},
   "source": [
    "1. Let $X_1$ and $X_2$ be jointly continuous random variables with probability denisty function $f_{X_1,X_2}$. Let $Y_1=X_1+X_2$ and $Y_2=X_1-X_2$. Find the joint density of $Y_1$ and $Y_2$ in terms of $f_{X_1,X_2}$.\n",
    "\n",
    "$$Y_1=g_1(X_1,X_2)=X_1+X_2\\qquad Y_2=g_2(X_1,X_2)=X_1-X_2$$\n",
    "\n",
    "$$x_1=y_1-x_2\\qquad x_1=y_2+x_2$$\n",
    "\n",
    "$$y_1-x_2=y_2+x_2$$\n",
    "\n",
    "$$y_1-y_2=2x_2$$\n",
    "\n",
    "$$x_2=\\frac{y_1+y_2}{2}$$\n",
    "\n",
    "$$x_1=y_1-x_2=y_1-\\frac{y_1+y_2}{2}$$\n",
    "\n",
    "$$J(x_1,x_2)=\\left|\\begin{matrix}\\frac{\\partial g_1}{\\partial x_1}&\\frac{\\partial g_1}{\\partial x_2}\\\\\\frac{\\partial g_2}{\\partial x_1}&\\frac{\\partial g_2}{\\partial x_2}\\end{matrix}\\right|=\\left|\\begin{matrix}1&1\\\\1&=1\\end{matrix}\\right|=1\\cdot-1-1\\cdot1=-2$$\n",
    "\n",
    "$$f_{Y_1,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)(x_1,x_2)|J(x_1,x_2)|^{-1}$$\n",
    "\n",
    "$$=f_{X_1,X_2}\\left(\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2}\\right)\\cdot|-2|^{-1}$$\n",
    "\n",
    "$$=\\frac{1}{2}\\cdot f_{X_1,X_2}\\left(\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2}\\right)$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6037579",
   "metadata": {},
   "source": [
    "2. Let $X$ and $Y$ denote the coordinates of a point uniformly chosen in the circle of radius 1 centered at the origin. That is, their joint density is\n",
    "\n",
    "$$f(x,y)=\\frac{1}{\\pi}\\quad x^2+y^2\\le1$$\n",
    "\n",
    "Find the joint density function of the polar coordinates $R=\\sqrt{X^2+Y^2}$ and $\\theta=\\tan^{-1}\\left(\\frac{Y}{X}\\right)$.\n",
    "\n",
    "Recall that $x=r\\cdot\\cos(\\theta)$, $y=r\\cdot\\sin(\\theta)$, $r=\\sqrt{x^2+y^2}$, and $\\theta=\\tan^{-1}(\\frac{x}{y})$\n",
    "\n",
    "Moreover,\n",
    "\n",
    "$$\\frac{\\partial g_1}{\\partial x}=\\frac{1}{2}(x^2+y^2)^{-\\frac{1}{2}}\\cdot 2x=\\frac{x}{\\sqrt{x^2+y^2}}\\qquad\\frac{\\partial g_1}{\\partial y}=\\frac{y}{\\sqrt{x^2+y^2}}$$\n",
    "\n",
    "$$\\frac{\\partial g_2}{\\partial x}=\\frac{1}{1+\\left(\\frac{y}{x}\\right)^2}\\cdot\\left(\\frac{-y}{x^2}\\right)=\\frac{-y}{x^2+y^2}$$\n",
    "\n",
    "$$\\frac{\\partial g_2}{\\partial y}=\\frac{1}{1+\\left(\\frac{y}{x}\\right)^2}\\cdot\\frac{1}{x}=\\frac{1}{x(1+\\frac{y^2}{x^2})}=\\frac{1}{x+\\frac{y^2}{x}}=\\frac{x}{x^2+y^2}$$\n",
    "\n",
    "$$J(x,y)=\\left|\\begin{matrix}\\frac{x}{\\sqrt{x^2+y^2}}&\\frac{y}{\\sqrt{x^2+y^2}}\\\\\\frac{-y}{\\sqrt{x^2+y^2}}&\\frac{x}{\\sqrt{x^2+y^2}}\\end{matrix}\\right|=\\frac{1}{r}$$\n",
    "\n",
    "$$\\implies f_{Y_1,Y_2}(y_1,y_2)=\\underbrace{f_{X,Y}(r\\cdot\\cos(\\theta),r\\cdot\\sin(\\theta))}_{\\underset{0\\le r\\le1,0\\le\\theta\\le2\\pi}{\\frac{1}{\\pi}\\text{ for }(r\\cdot\\cos(\\theta))^2+(r\\cdot\\sin(\\theta))^2\\le1}}\\cdot|\\frac{1}{r}|^{-1}$$\n",
    "\n",
    "$$=\\frac{r}{\\pi}$$\n",
    "\n",
    "$$=2r\\cdot\\frac{1}{2\\pi}\\implies\\theta\\sim\\text{Uniform}(0,2\\pi)\\text{ and }R\\text{ has density }f_R(r)=2r$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9f607",
   "metadata": {},
   "source": [
    "3. If $X$ and $Y$ are independent gamma random variables with parameters $(\\ell,\\lambda)$ and $(\\beta,\\lambda)$, respectively, compute the joint density of $U=X+Y$ and $V=X/(X+Y)$. \n",
    "\n",
    "$$u=g_1(x,y)=x+y\\qquad v=g_2(x,y)=\\frac{x}{x+y}$$\n",
    "\n",
    "1. $u=x+y$\n",
    "\n",
    "2. $v=\\frac{x}{x+y}=\\frac{x}{u}\\implies x=ub\\implies y=u-x=u-uv=u(1-v)$\n",
    "\n",
    "$$J(x,y)=\\left|\\begin{matrix}1&1\\\\\\frac{y}{(x+y)^2}&\\frac{-x}{(x+y)^2}\\end{matrix}\\right|=\\frac{-1}{u}$$\n",
    "\n",
    "$$f_{XY}(x,y)=\\frac{\\lambda e^{-\\lambda x}(\\lambda x)^{\\ell-1}}{\\Gamma(\\ell)}\\cdot\\frac{\\lambda e^{-\\lambda y}(\\lambda y)^{\\beta-1}}{\\Gamma(\\beta)}$$\n",
    "\n",
    "$$=\\frac{\\lambda^{\\ell+\\beta}}{\\Gamma(\\ell)\\Gamma(\\beta)}e^{-\\lambda(x+y)}x^{\\ell-1}y^{\\beta-1}$$\n",
    "\n",
    "$$f_{u,v}(u,v)=f_{XY}(uv, u(1-v))\\cdot u$$\n",
    "\n",
    "$$=\\frac{\\lambda^{\\ell+\\beta}}{\\Gamma(\\ell)\\Gamma(\\beta)}e^{-\\lambda(uv+u-uv)}(uv)^{\\ell-1}(u(1-v))^{\\beta-1}\\cdot u$$\n",
    "\n",
    "$$=\\frac{\\lambda^{\\ell+\\beta}}{\\Gamma(\\ell)\\Gamma(\\beta)}e^{-\\lambda u}u^{\\ell-1}v^{\\ell-1}u^{\\beta-1}(1-v)^{\\beta-1}\\cdot u$$\n",
    "\n",
    "$$=\\frac{\\lambda^{\\ell+\\beta}}{\\Gamma(\\ell)\\Gamma(\\beta)}e^{-\\lambda u}u^{\\ell+\\beta-1}v^{\\ell-1}(1-v)^{\\beta-1}$$\n",
    "\n",
    "$$=\\frac{\\lambda^{\\ell+\\beta}}{\\Gamma(\\ell)\\Gamma(\\beta)}e^{-\\lambda u}u^{\\ell+\\beta-1}v^{\\ell-1}(1-v)^{\\beta-1}$$\n",
    "\n",
    "$$=\\frac{\\lambda e^{-\\lambda u}(\\lambda u)^{\\ell+\\beta-1}}{\\Gamma(\\ell+\\beta)}\\cdot\\frac{v^{\\ell-1}(1-v)^{\\beta-1}\\Gamma(\\ell+\\beta)}{\\Gamma(\\ell)\\Gamma(\\beta)}$$\n",
    "\n",
    "Hence, $u=x+y$ and $v=\\frac{x}{x+y}$ are independent, where $X+Y\\sim\\text{Gamma}(\\ell+\\beta,\\lambda)$ and $\\frac{X}{X+Y}\\sim\\text{Beta}(\\ell, \\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ddaa68",
   "metadata": {},
   "source": [
    "4. Suppose you have an urn containing 1 red ball and 2 white balls. Draw out balls, one at a time without replacement, and note the color. Define\n",
    "\n",
    "$$X_i=\\begin{cases}1&\\text{ball }i\\text{ is red}\\\\0&\\text{else}\\end{cases}$$\n",
    "\n",
    "Are the random variables $X_1,X_2,X_3$ exchangeable?\n",
    "\n",
    "If the arguments for $P(X_1=x_1,X_2=x_2,X_3=x_3)$ are anything other than two 0's and one 1, regardless of the order, the probability is zero. So, we must only check arguments that are permutations of $(1,0,0)$.\n",
    "\n",
    "$$P(X_1=1,X_2=0,X_3=0)=\\frac{1}{3}\\cdot1\\cdot1=\\frac{1}{3}$$\n",
    "\n",
    "$$P(X_1=0,X_2=1,X_3=0)=\\frac{2}{3}\\cdot\\frac{1}{2}\\cdot1=\\frac{1}{3}$$\n",
    "\n",
    "$$P(X_1=0,X_2=0,X_3=1)=\\frac{2}{3}\\cdot\\frac{1}{2}\\cdot1=\\frac{1}{3}$$\n",
    "\n",
    "Therefore, the random variables $X_1,X_2,X_3$ are exchangeable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e402f7e",
   "metadata": {},
   "source": [
    "5. Show that if $X_1,\\ldots,X_n$ are independent and identically distributed, then $X_1,\\ldots,X_n$ are exchangeable.\n",
    "\n",
    "Let $f(x)$ (or $p(x)$) bet he PDF (or PMF) for any of the $X_i$. They are i.i.d., so they all follow the same PDF (PMF). The joint PDF (PMF) is\n",
    "\n",
    "$$f_{X_1,X_2,\\ldots,X_n}(x_1,x_2,\\ldots,x_n)=f(x_1)f(x_2)\\ldots f(x_n)$$\n",
    "\n",
    "Since we can multiple the terms on the right-hand side in any order, the left-hand side is clearly symmetric in its arguments. Thus, $X_1,X_2,\\ldots,X_n$ are exchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e114f8",
   "metadata": {},
   "source": [
    "5. Suppose that an urn initially contains $n$ red and $m$ blue balls. At each stage, a ball is randomly chosen, its color is noted, and the it is replaced along with another ball of the same color. Let $X_i=1$ if the $i\\text{th}$ ball selected is red and let it equal $0$ if the $i\\text{th}$ ball is blue, $i\\ge1$. Show that, for any value of $k$, the random variables $X_1,\\ldots,X_k$ are exchangeable.\n",
    "\n",
    "Start with some examples\n",
    "\n",
    "$$P(X_1=1,X_2=1,X_3=0,X_4=1,X_5=0)$$\n",
    "\n",
    "$$=\\frac{n}{n+m}\\cdot\\frac{n+1}{n+m+1}\\cdot\\frac{m}{n+m+2}\\cdot\\frac{n+2}{n+m+3}\\cdot\\frac{m+1}{n+m+4}$$\n",
    "\n",
    "$$=\\frac{n(n+1)(n+2)m(m+1)}{(n+m)(n+m+1)(n+m+2)(n+m+3)(n+m+4)}$$\n",
    "\n",
    "$$P(X_1=0,X_2,X_3=0,X_4,X_5=1)$$\n",
    "\n",
    "$$=\\frac{m}{n+m}\\cdot\\frac{n}{n+m+1}\\cdot\\frac{m+1}{n+m+2}\\cdot\\frac{n+1}{n+m+3}\\cdot\\frac{n+2}{n+m+4}$$\n",
    "\n",
    "$$=\\frac{n(n+1)(n+2)m(m+1)}{(n+m)(n+m+1)(n+m+2)(n+m+3)(n+m+4)}$$\n",
    "\n",
    "By the same reasoning, for any sequence $x_1,\\ldots,x_k$ that contains $r$ ones and $k-r$ zeros, we have:\n",
    "\n",
    "$$P(X_1=x_1,X_2,\\ldots,X_k=x_k)$$\n",
    "\n",
    "$$=\\frac{n(n+1)\\ldots(n+r-1)m(m+1)\\ldots(m+k-r-1)}{(n+m)(n+m+1)\\ldots(n+m+k-1)}$$\n",
    "\n",
    "$\\implies X_1,\\ldots,X_k$ are exchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8ddce",
   "metadata": {},
   "source": [
    "6. Let $X_1,X_2,\\ldots,X_n$ be independent uniform (0,1) random variables, and denote their order statistics by $X_{(1)},X_{(2)},\\ldots,X_{(n)}$. Tha is, $X_{(j)}$ is the $j\\text{th}$ smallest of $X_1,X_2,\\ldots,X_n$. Also, let \n",
    "\n",
    "$$Y_1=X_{(1)}$$\n",
    "\n",
    "$$Y_i=X_{(i)}-X_{(i-1)},\\quad i=2,\\ldots,n$$\n",
    "\n",
    "Show that $Y_1,\\ldots,Y_n$ are exchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f07a3",
   "metadata": {},
   "source": [
    "6. ALong a road 1 mile long are 3 people \"distirubted at random.\" Find the probabilty that no 2 people are less than a distance of $d$ miles apart when $d\\ge\\frac{1}{2}$.\n",
    "\n",
    "Assume that \"distributed at random\" means that the positions of the 3 people are independnent and uniformly distributed over the road. If $X_i$ denotes the postion of the $i\\text{th}$ person, then the desired probability is $P(X_{(i)}>X_{(i-1)}+d;i=2,3)$\n",
    "\n",
    "As $f_{X_{(1)},X_{(2)},X_{(3)}}(x_1,x_2,x_3)=3!\\cdot1\\cdot1\\cdot1=3!$ for $0<x_1<x_2<x_3<1$ it follows that\n",
    "\n",
    "$$P(X_{(1)}>X_{(i-1)}+d;i=2,3)=\\underset{x_i>x_{i-1}+d}{\\int\\int\\int}3!dx_1dx_2dx_3$$\n",
    "\n",
    "$$=3!\\int_{x_1=0}^{1-2d}\\int_{x_2=x_1+d}^{1-d}\\int_{x_3=x_2+d}^1dx_3dx_2dx_1$$\n",
    "\n",
    "$$=3!\\cdot\\int_{x_1=0}^{1-2d}\\int_{x_2=x_1+d}^{1-d}\\int_{x_3=x_2+d}^1dx_3dx_2dx_1$$\n",
    "\n",
    "$$=6\\int_{x_1=0}^{1-2d}\\int_{x_2=x_1+d}^{1-d}(1-d-x_2)dx_2dx_1$$\n",
    "\n",
    "$$=6\\int_{x_1=0}^{1-2d}\\left[x_2-dx_2-\\frac{x_2^2}{2}\\right]_{x_1+d}^{1-d}dx_1$$\n",
    "\n",
    "$$3\\cdot(-\\frac{1}{3}(2d-1)^3)=-(2d-1)^3=(1-2d)^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340007e3",
   "metadata": {},
   "source": [
    "7. When a sample of $2n+1$ random variables (that is, when $2n+1$ independent and identically distributed random variable) is observed, the $(n+1)\\text{th}$ smallest is called the sample median. If a sample of size $3$ from a unifrom distribution over (0,1) is observed, find the probability that the sample median is between $\\frac{1}{4}$ and $\\frac{3}{4}$.\n",
    "\n",
    "The density of $X_(2)$ (which corresponds to the median) is given by\n",
    "\n",
    "$$f_{X_(2)}=\\frac{3!}{1!\\cdot1!}(x)^{2-1}(1-x)^{3-2}\\cdot1=6x(1-x)\\quad\\text{for }0\\le x\\le 1$$\n",
    "\n",
    "$$\\implies P\\left(\\frac{1}{u}\\le X_{(2)}\\le\\frac{3}{4}\\right)=6\\int_{\\frac{1}{4}}^{\\frac{3}{4}}x(1-x)dx=5\\cdot\\int_{\\frac{1}{4}}^{\\frac{3}{4}}(x-x^2)dx$$\n",
    "\n",
    "$$=b\\left[\\frac{x^2}{2}-\\frac{x^3}{3}\\right]_{\\frac{1}{4}}^{\\frac{3}{4}}=\\frac{11}{16}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7885e640",
   "metadata": {},
   "source": [
    "8. Claim amounts for wind damage to insured homes are mutually independent random variables with common density function\n",
    "\n",
    "$$f(x)=\\begin{cases}\\frac{3}{x^4}&x>4\\\\0&\\text{else}\\end{cases}$$\n",
    "\n",
    "where $x$ is the amouynt of a claim in thousands. Suppose 3 such claims will be made. Calculate the expected value of the largest of the three claims.\n",
    "\n",
    "$$F_X(x)=\\int_1^x\\frac{3}{t^4}dt=[-t^{-3}]_1^x=1-x^{-3},\\ x>1$$\n",
    "\n",
    "Let $X_1, X_2,X_3$ denote the three claims made. Then, \n",
    "\n",
    "$$F_{X_{(3)}}(y)=P(X_{(3)}\\le y)=\\begin{pmatrix}3\\\\3\\end{pmatrix}[1-y^{-3}]^3=(1-y^{-3})^3$$\n",
    "\n",
    "$$\\implies f_{X_(3)}(y)=3\\cdot(1-y^{-3})^2\\cdot(3y^{-u})=g(1-y^{-3})^2y^{-4}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "E[X_{(3)}]&=\\int_1^{\\infty}y\\cdot 9\\cdot(1-y^{-3})^2\\cdot y^{-4}dy\\\\\n",
    "&=9\\cdot\\int_1^{\\infty}y^{-3}(1-2\\cdot y^{-3}+y^{-6})dy\\\\\n",
    "&=9\\cdot\\int_{1}^{\\infty}(y^{-3}-2y^{-6}+y^{-9})dy\\\\\n",
    "&=9\\cdot\\left[-\\frac{1}{2}y^{-2}+\\frac{2}{5}y^{-5}-\\frac{1}{8}y^{-8}\\right]_1^{\\infty}\\\\\n",
    "&=9(0+\\frac{1}{2}-\\frac{2}{5}+\\frac{1}{8})\\\\\n",
    "&=9\\cdot\\frac{9}{40}\\\\\n",
    "&=\\frac{81}{40}\\\\\n",
    "&\\approx2.025\\cdot1,000=2025\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f342eb",
   "metadata": {},
   "source": [
    "9. The joint density function of $X$ and $Y$ is given by\n",
    "\n",
    "$$f(x,y)=xe^{-x(y+1)}\\quad x>0,y>0$$\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;(a) Find the conditional density of $X$, given $Y=y$, and that of $Y$, given $X=x$.\n",
    "\n",
    "$$f_{X|Y}(x|y)=\\frac{f(x,y)}{f_Y(y)}=\\frac{xe^{-x(y+1)}}{\\int_{0}^{\\infty}xe^{-x(y+1)}dx}$$\n",
    "\n",
    "$$=\\frac{xe^{-x(y+1)}}{\\frac{1}{(1+y)^2}}=(y+1)^2xe^{-x(y+1)},\\quad x>0$$\n",
    "\n",
    "$$f_{Y|X}(y|x)=\\frac{xe^{-x(y+1)}}{\\int_{0}^{\\infty}xe^{-x(y+1)}dy}$$\n",
    "\n",
    "$$=\\frac{xe^{-x(y+1)}}{e^{-x}}=xe^{-xy},\\quad y>0$$\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;(b) Find the density function of $Z=XY$.\n",
    "\n",
    "$$P(Z\\le z)=P(XY\\le z)=P(X\\le\\frac{z}{Y})=F_X(\\frac{z}{Y})$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
