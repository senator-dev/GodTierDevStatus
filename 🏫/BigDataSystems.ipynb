{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational vs. Unconditional Security: A Comparison\n",
    "\n",
    "## Computational Security\n",
    "\n",
    "Computational security is based on the practical difficulty of solving certain mathematical problems within reasonable time constraints.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Security depends on the computational limitations of attackers\n",
    "- Can theoretically be broken with sufficient computing power\n",
    "- Relies on unproven mathematical assumptions\n",
    "- Security measured by estimated computational effort to break\n",
    "- Forms the basis for most modern cryptographic systems\n",
    "\n",
    "**Examples:**\n",
    "- RSA encryption (based on difficulty of factoring large numbers)\n",
    "- AES encryption (based on computational complexity)\n",
    "- Diffie-Hellman key exchange\n",
    "- Elliptic Curve Cryptography\n",
    "\n",
    "## Unconditional Security\n",
    "\n",
    "Unconditional security provides mathematical guarantees independent of an attacker's computational resources.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Security holds even against attackers with unlimited computing power\n",
    "- Based on information-theoretic principles\n",
    "- Cannot be broken by computational means alone\n",
    "- Attacker lacks sufficient information to break the system\n",
    "- Often impractical for widespread implementation\n",
    "\n",
    "**Examples:**\n",
    "- One-time pad encryption\n",
    "- Quantum key distribution\n",
    "- Secret sharing schemes\n",
    "- Some forms of authentication codes\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "While unconditional security offers stronger theoretical guarantees, computational security dominates practical applications due to:\n",
    "\n",
    "1. **Implementation practicality:** Unconditionally secure systems often have strict requirements (e.g., one-time pads require pre-shared keys equal in length to the message)\n",
    "   \n",
    "2. **Key management:** Many unconditionally secure systems have challenging key distribution and management requirements\n",
    "   \n",
    "3. **Efficiency:** Computationally secure systems typically offer better performance characteristics\n",
    "   \n",
    "4. **Adequacy:** Well-designed computationally secure systems can provide security that is practically sufficient for most applications\n",
    "\n",
    "In real-world scenarios, security systems are typically designed to be computationally secure against current and near-future technologies, with appropriate key sizes and algorithm selections to ensure adequate security margins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional vs. Public Key Encryption: A Comparison\n",
    "\n",
    "## Conventional (Symmetric) Encryption\n",
    "\n",
    "Conventional encryption uses the same key for both encryption and decryption processes.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Single shared secret key for both encryption and decryption\n",
    "- Both sender and receiver must possess the same key\n",
    "- Generally faster and more efficient than public key encryption\n",
    "- Suitable for encrypting large volumes of data\n",
    "- Key distribution is a significant challenge\n",
    "\n",
    "**Examples:**\n",
    "- AES (Advanced Encryption Standard)\n",
    "- DES (Data Encryption Standard) and 3DES\n",
    "- Blowfish\n",
    "- ChaCha20\n",
    "\n",
    "## Public Key (Asymmetric) Encryption\n",
    "\n",
    "Public key encryption uses mathematically related but different keys for encryption and decryption.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Uses key pairs: public key (for encryption) and private key (for decryption)\n",
    "- Public key can be freely distributed while private key remains secret\n",
    "- Computationally more intensive than symmetric encryption\n",
    "- Solves the key distribution problem inherent in symmetric encryption\n",
    "- Often used for secure key exchange, digital signatures, and authentication\n",
    "\n",
    "**Examples:**\n",
    "- RSA (Rivest-Shamir-Adleman)\n",
    "- ECC (Elliptic Curve Cryptography)\n",
    "- ElGamal encryption\n",
    "- Diffie-Hellman key exchange (for establishing shared secrets)\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "In practice, both encryption types are often used together in hybrid cryptosystems:\n",
    "\n",
    "1. **Key Exchange:** Public key encryption is used to securely exchange a symmetric key.\n",
    "   \n",
    "2. **Bulk Data Encryption:** The symmetric key is then used for efficient encryption of the actual data.\n",
    "\n",
    "**Use Cases by Type:**\n",
    "\n",
    "| Symmetric (Conventional) | Asymmetric (Public Key) |\n",
    "|--------------------------|-------------------------|\n",
    "| File encryption | Digital signatures |\n",
    "| Database encryption | Certificate-based authentication |\n",
    "| Session encryption (TLS/SSL data) | Key exchange |\n",
    "| Disk encryption | Identity verification |\n",
    "| Fast encryption of large datasets | Non-repudiation services |\n",
    "\n",
    "## Performance Comparison\n",
    "\n",
    "| Factor | Conventional (Symmetric) | Public Key (Asymmetric) |\n",
    "|--------|--------------------------|-------------------------|\n",
    "| Speed | Faster (10-1000x) | Slower |\n",
    "| Key Size | Smaller (128-256 bits typical) | Larger (2048+ bits RSA, 256+ bits ECC) |\n",
    "| Security per Bit | Higher | Lower |\n",
    "| Key Management | More complex distribution | Easier distribution, complex management |\n",
    "| Resource Usage | Lower CPU/memory requirements | Higher CPU/memory requirements |\n",
    "\n",
    "## Security Considerations\n",
    "\n",
    "- **Symmetric systems:** Security primarily depends on keeping the key secret and using strong algorithms.\n",
    "\n",
    "- **Asymmetric systems:** Security relies on computational difficulty of mathematical problems and proper implementation.\n",
    "\n",
    "Both approaches are essential components of modern cryptographic systems, each addressing different aspects of the security challenges in digital communications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! GRAPH AT BOTTOM OF SLIDE 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Performance Computing (HPC) Architectures: A Comprehensive Guide\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to HPC](#introduction-to-hpc)\n",
    "2. [Shared Memory Architectures](#shared-memory-architectures)\n",
    "   - [Symmetric Multiprocessing (SMP)](#symmetric-multiprocessing-smp)\n",
    "   - [Non-Uniform Memory Access (NUMA)](#non-uniform-memory-access-numa)\n",
    "   - [Cache-Coherent NUMA (ccNUMA)](#cache-coherent-numa-ccnuma)\n",
    "3. [Distributed Memory Architectures](#distributed-memory-architectures)\n",
    "   - [Distributed Memory Multicomputers](#distributed-memory-multicomputers)\n",
    "   - [Distributed Memory Multiprocessors](#distributed-memory-multiprocessors)\n",
    "   - [Massive Parallel Processing (MPP)](#massive-parallel-processing-mpp)\n",
    "4. [Vector Processing Architectures](#vector-processing-architectures)\n",
    "   - [Vector Computers](#vector-computers)\n",
    "   - [SIMD Extensions](#simd-extensions)\n",
    "5. [Hybrid Architectures](#hybrid-architectures)\n",
    "   - [Clusters](#clusters)\n",
    "   - [Constellations](#constellations)\n",
    "   - [Grid Computing](#grid-computing)\n",
    "6. [Specialized Architectures](#specialized-architectures)\n",
    "   - [GPU Computing](#gpu-computing)\n",
    "   - [FPGA-based Systems](#fpga-based-systems)\n",
    "   - [Quantum Computing](#quantum-computing)\n",
    "7. [Memory Hierarchies in HPC](#memory-hierarchies-in-hpc)\n",
    "8. [Interconnect Technologies](#interconnect-technologies)\n",
    "9. [Performance Metrics and Benchmarking](#performance-metrics-and-benchmarking)\n",
    "10. [Programming Models for HPC](#programming-models-for-hpc)\n",
    "11. [Current Trends and Future Directions](#current-trends-and-future-directions)\n",
    "12. [Glossary of HPC Terms](#glossary-of-hpc-terms)\n",
    "\n",
    "## Introduction to HPC\n",
    "\n",
    "High-Performance Computing (HPC) refers to the practice of aggregating computing power in a way that delivers much higher performance than one could get out of a typical desktop computer or workstation in order to solve large problems in science, engineering, or business. The architectures that enable HPC have evolved significantly over time, driven by technological advancements and changing computational needs.\n",
    "\n",
    "HPC systems are characterized by:\n",
    "- High computational capacity\n",
    "- Large memory capacity and bandwidth\n",
    "- Fast interconnection networks\n",
    "- Efficient I/O subsystems\n",
    "- Specialized software environments\n",
    "\n",
    "This document provides a comprehensive overview of various HPC architectures, examining their characteristics, advantages, limitations, and typical use cases.\n",
    "\n",
    "## Shared Memory Architectures\n",
    "\n",
    "Shared memory architectures are characterized by multiple processors accessing a common memory space. They allow for relatively straightforward programming as all processors can directly access all memory locations.\n",
    "\n",
    "### Symmetric Multiprocessing (SMP)\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Symmetric Multiprocessing (SMP) is one of the most straightforward parallel architectures, characterized by:\n",
    "\n",
    "- Multiple identical processors connected to a single, shared main memory\n",
    "- Uniform memory access (UMA) - all processors have equal access time to all memory locations\n",
    "- Centralized shared memory accessible via a common bus or crossbar switch\n",
    "- A single copy of the operating system managing all processors\n",
    "\n",
    "![SMP Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Memory Access**: Uniform access times for all processors to all memory locations\n",
    "- **Memory Coherence**: Maintained through bus snooping or directory-based protocols\n",
    "- **Scalability**: Limited, typically up to 32-64 processors\n",
    "- **Programming Model**: Shared memory programming (e.g., OpenMP, POSIX threads)\n",
    "- **Load Balancing**: Straightforward due to homogeneous processors and uniform memory access\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Simple to program compared to distributed memory systems\n",
    "- Efficient for problems with unpredictable memory access patterns\n",
    "- Good performance for small to medium-scale parallel applications\n",
    "- Lower communication overhead for shared data\n",
    "- Effective use of shared caches for cooperating processes\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Memory bandwidth becomes a bottleneck as the number of processors increases\n",
    "- Bus contention limits scalability\n",
    "- Cache coherence overhead increases with system size\n",
    "- Limited fault tolerance (failure of the shared memory affects all processors)\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Traditional multi-socket server motherboards\n",
    "- Early SGI Challenge and Onyx systems\n",
    "- Sun Enterprise servers\n",
    "- Modern multi-core processors (on a chip scale)\n",
    "\n",
    "### Non-Uniform Memory Access (NUMA)\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Non-Uniform Memory Access (NUMA) extends the shared memory model to overcome the scalability limitations of SMP by:\n",
    "\n",
    "- Organizing memory in multiple nodes, each with local memory and processors\n",
    "- Maintaining a global address space accessible by all processors\n",
    "- Providing faster access to local memory than to remote memory\n",
    "- Using specialized interconnects to connect multiple memory nodes\n",
    "\n",
    "![NUMA Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Memory Access**: Non-uniform access times - local memory accesses are faster than remote memory accesses\n",
    "- **Memory Organization**: Physically distributed but logically shared\n",
    "- **Node Structure**: Each node contains one or more processors and local memory\n",
    "- **Scalability**: Better than SMP, typically up to hundreds of processors\n",
    "- **Memory Hierarchy**: Additional level of memory hierarchy (local vs. remote)\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Better scalability than SMP systems\n",
    "- Reduced memory contention\n",
    "- Improved performance for applications with good memory locality\n",
    "- Maintains programming simplicity of shared memory model\n",
    "- Higher aggregate memory bandwidth\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Performance heavily dependent on memory access patterns\n",
    "- \"NUMA penalties\" when accessing remote memory\n",
    "- Complex memory management required for optimal performance\n",
    "- Increased latency for remote memory accesses\n",
    "- Challenging to achieve balanced performance across applications\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- SGI Origin series\n",
    "- HP/Convex Exemplar\n",
    "- IBM POWER systems with NUMA support\n",
    "- AMD Opteron-based multi-socket servers\n",
    "- Intel Xeon servers with QuickPath Interconnect or UltraPath Interconnect\n",
    "\n",
    "### Cache-Coherent NUMA (ccNUMA)\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Cache-Coherent NUMA (ccNUMA) enhances the NUMA architecture by adding hardware-supported cache coherence mechanisms:\n",
    "\n",
    "- Maintains consistency between multiple caches across NUMA nodes\n",
    "- Implements directory-based or snooping protocols to track cache line states\n",
    "- Provides transparent access to remote memory while maintaining coherence\n",
    "- Combines NUMA's distributed memory benefits with SMP's programming simplicity\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Cache Coherence**: Hardware-maintained coherence across all nodes\n",
    "- **Directory Structures**: Often uses directory-based protocols to track cache line states\n",
    "- **Memory Consistency**: Various consistency models supported (sequential, release, etc.)\n",
    "- **Transparency**: Memory distribution is transparent to software\n",
    "- **Scalability**: Better than basic NUMA, with implementations scaling to hundreds of processors\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Simplified programming model with automatic cache coherence\n",
    "- Better performance than basic NUMA for shared data\n",
    "- Efficient for both computation-intensive and data-sharing workloads\n",
    "- Balances distributed memory performance with shared memory programmability\n",
    "- Good compromise architecture for many HPC applications\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Complex hardware implementation\n",
    "- Coherence protocol overhead can limit scalability\n",
    "- Directory size limitations can impact large-scale systems\n",
    "- Still subject to NUMA effects for performance\n",
    "- Higher cost compared to non-coherent systems\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- SGI Origin and Altix systems\n",
    "- Bull NovaScale servers\n",
    "- HP Superdome\n",
    "- Larger AMD EPYC and Intel Xeon multi-socket systems\n",
    "- Cray XC series (node level)\n",
    "\n",
    "## Distributed Memory Architectures\n",
    "\n",
    "Distributed memory architectures lack a common address space across processors. Each processor has its own local memory, and communication between processors occurs through explicit message passing.\n",
    "\n",
    "### Distributed Memory Multicomputers\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Distributed Memory Multicomputers connect complete computers (with their own processor, memory, and potentially I/O) through an interconnection network:\n",
    "\n",
    "- Each node has private memory not directly accessible to other nodes\n",
    "- Explicit message passing is required for inter-node communication\n",
    "- No hardware support for shared memory abstraction\n",
    "- Complete software environment on each node\n",
    "\n",
    "![Distributed Memory Multicomputer Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Memory Access**: No direct access to other nodes' memory\n",
    "- **Communication Model**: Explicit message passing (e.g., using MPI)\n",
    "- **Node Independence**: Each node can operate independently\n",
    "- **Scalability**: Excellent, can scale to thousands of nodes\n",
    "- **Programming Complexity**: Higher than shared memory systems\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Excellent scalability\n",
    "- Cost-effectiveness (can use commodity hardware)\n",
    "- No cache coherence overhead\n",
    "- Better fault isolation (node failures don't necessarily affect others)\n",
    "- High aggregate memory bandwidth\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- More complex programming model\n",
    "- Communication overhead for data exchange\n",
    "- Data partitioning and load balancing challenges\n",
    "- Potential for high latency in communication\n",
    "- Difficult to implement irregular or dynamic algorithms efficiently\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Early Beowulf clusters\n",
    "- Commodity clusters built with standard servers\n",
    "- IBM SP series\n",
    "- Custom-built compute farms\n",
    "- Cloud-based HPC clusters\n",
    "\n",
    "### Distributed Memory Multiprocessors\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Distributed Memory Multiprocessors are tightly integrated systems where:\n",
    "\n",
    "- Processing elements share an interconnection network but have private memories\n",
    "- Hardware is specifically designed for HPC workloads\n",
    "- The network topology and hardware are optimized for efficient communication\n",
    "- System software provides a unified environment across nodes\n",
    "- Specialized operating system or runtime environment may be used\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Integration Level**: Higher than multicomputers, with specialized hardware\n",
    "- **Network Design**: Custom high-performance interconnects\n",
    "- **System Software**: Unified job scheduling and resource management\n",
    "- **Architecture Specialization**: Often customized for specific workload domains\n",
    "- **Performance Focus**: Balanced computation and communication capabilities\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Better communication performance than general multicomputers\n",
    "- Optimized hardware/software co-design\n",
    "- Higher reliability than commodity clusters\n",
    "- More predictable performance characteristics\n",
    "- Better system management tools\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Higher cost than commodity clusters\n",
    "- Potential vendor lock-in\n",
    "- Less flexible than general-purpose systems\n",
    "- Specialized programming environment may be required\n",
    "- Typically less accessible than commodity systems\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Cray T3D and T3E\n",
    "- IBM Blue Gene series\n",
    "- Some configurations of Fujitsu systems\n",
    "- Dedicated HPC systems from vendors like HPE, Dell, and Lenovo\n",
    "- Custom supercomputer designs for specific national labs\n",
    "\n",
    "### Massive Parallel Processing (MPP)\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Massive Parallel Processing systems represent the largest scale of distributed memory systems:\n",
    "\n",
    "- Thousands to millions of processing elements\n",
    "- Highly specialized interconnect networks\n",
    "- Sophisticated system software for management\n",
    "- Focus on scalability to extreme node counts\n",
    "- Often purpose-built for specific scientific domains\n",
    "\n",
    "![MPP Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Scale**: Extreme node counts (thousands to millions)\n",
    "- **Specialization**: Often designed for specific application domains\n",
    "- **System Management**: Sophisticated job scheduling and resource allocation\n",
    "- **Power Efficiency**: Critical design consideration\n",
    "- **Fault Tolerance**: Essential due to the large component count\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Ultimate computational scaling capability\n",
    "- Ability to solve the largest computational problems\n",
    "- Potential for breakthrough results in science and engineering\n",
    "- Economies of scale in some aspects of design\n",
    "- Pushing boundaries of parallel computing\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Enormous cost and complexity\n",
    "- Significant power requirements\n",
    "- Programming challenges at extreme scale\n",
    "- Reliability challenges due to component count\n",
    "- Specialized knowledge required for effective use\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Sunway TaihuLight\n",
    "- Fujitsu Fugaku\n",
    "- Summit and Sierra systems (IBM+NVIDIA)\n",
    "- Frontier and Aurora exascale systems\n",
    "- Tianhe series supercomputers\n",
    "\n",
    "## Vector Processing Architectures\n",
    "\n",
    "Vector processing architectures are specialized for operations that can be applied to multiple data elements simultaneously, particularly useful for scientific simulations and data analysis.\n",
    "\n",
    "### Vector Computers\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Vector computers are designed specifically to handle vector operations efficiently:\n",
    "\n",
    "- Specialized vector registers holding multiple data elements\n",
    "- Vector instructions operating on entire vectors\n",
    "- Pipelined execution of vector operations\n",
    "- High memory bandwidth to feed vector units\n",
    "- Optimized for regular, structured computations\n",
    "\n",
    "![Vector Computer Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Vector Registers**: Wide registers holding multiple data elements\n",
    "- **Vector Instructions**: Single instructions operating on multiple data elements\n",
    "- **Memory System**: Designed for high bandwidth streaming access\n",
    "- **Pipelining**: Deep pipelines for vector operations\n",
    "- **Vectorization**: Compiler and hardware support for identifying vector operations\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Excellent performance for regular scientific computations\n",
    "- Efficient use of memory bandwidth\n",
    "- Reduced instruction overhead\n",
    "- Highly predictable performance\n",
    "- Well-suited for many scientific and engineering problems\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Poor performance for scalar or irregular operations\n",
    "- Limited applicability to general-purpose computing\n",
    "- High cost of specialized hardware\n",
    "- Programming complexity for optimal vectorization\n",
    "- Diminishing returns for short vectors\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Classic Cray-1, Cray X-MP, Cray Y-MP\n",
    "- NEC SX series\n",
    "- Fujitsu VP series\n",
    "- Earth Simulator (Japan)\n",
    "- Modern vector extensions in conventional CPUs\n",
    "\n",
    "### SIMD Extensions\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Single Instruction, Multiple Data (SIMD) extensions add vector processing capabilities to conventional CPUs:\n",
    "\n",
    "- Vector registers and instructions integrated into standard CPU architecture\n",
    "- Fixed-width vector operations (e.g., 128, 256, or 512 bits)\n",
    "- Support for various data types and operations\n",
    "- Compiler and intrinsics support for programming\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Integration**: Part of conventional CPU architecture\n",
    "- **Vector Width**: Fixed-width vectors defined by the architecture\n",
    "- **Instruction Set**: Special vector instructions alongside scalar instructions\n",
    "- **Programming Model**: Mixture of automatic vectorization and explicit vector programming\n",
    "- **Performance Impact**: Significant speedup for suitable workloads\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Improved performance for vectorizable code without specialized systems\n",
    "- Cost-effective approach to vector processing\n",
    "- Wide availability in modern processors\n",
    "- Incremental adoption possible in existing code\n",
    "- Standardized programming interfaces (in some cases)\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Limited vector lengths compared to traditional vector computers\n",
    "- Restricted operations compared to full vector machines\n",
    "- Programming complexity for optimal utilization\n",
    "- Performance dependent on memory access patterns\n",
    "- Limited by CPU's memory bandwidth\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Intel AVX, AVX2, AVX-512 \n",
    "- ARM NEON and SVE\n",
    "- AMD 3DNow! and extensions\n",
    "- IBM AltiVec/VMX\n",
    "- RISC-V vector extensions\n",
    "\n",
    "## Hybrid Architectures\n",
    "\n",
    "Hybrid architectures combine multiple paradigms to leverage the strengths of different approaches while mitigating their weaknesses.\n",
    "\n",
    "### Clusters\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Clusters connect multiple independent computers (nodes) to work as a unified system:\n",
    "\n",
    "- Nodes connected by a high-speed network\n",
    "- Each node typically has a multi-core processor (SMP or NUMA)\n",
    "- Distributed memory across nodes, shared memory within nodes\n",
    "- Standard network protocols and interfaces\n",
    "- Common job scheduling and resource management\n",
    "\n",
    "![Cluster Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Node Architecture**: Typically SMP or NUMA servers\n",
    "- **Interconnect**: Various options from Ethernet to specialized networks\n",
    "- **Programming Model**: Often hybrid MPI+OpenMP or similar\n",
    "- **Scalability**: Good to excellent, depending on interconnect and workload\n",
    "- **Cost Efficiency**: Often built from commodity components\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Excellent price/performance ratio\n",
    "- Flexibility in configuration and expansion\n",
    "- Familiar component technologies\n",
    "- Wide range of software support\n",
    "- Scalable in manageable increments\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Network performance can become a bottleneck\n",
    "- System management complexity\n",
    "- Potential reliability issues with commodity components\n",
    "- Programming complexity for hybrid models\n",
    "- Load balancing challenges\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Linux clusters in academia and industry\n",
    "- TOP500 supercomputer list dominated by clusters\n",
    "- Cloud-based HPC clusters\n",
    "- Beowulf and descendant architectures\n",
    "- Department-level computing resources\n",
    "\n",
    "### Constellations\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Constellations represent large-scale systems built from smaller, tightly-coupled subsystems:\n",
    "\n",
    "- Multiple smaller parallel systems connected into a larger whole\n",
    "- Hierarchical organization of computing resources\n",
    "- Often heterogeneous node types for different computation patterns\n",
    "- Multiple interconnect technologies at different hierarchy levels\n",
    "- Unified system software across the constellation\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Hierarchical Design**: Multiple levels of parallelism and communication\n",
    "- **Heterogeneity**: Often includes different types of compute resources\n",
    "- **Interconnect Hierarchy**: Faster connections within subsystems, different connections between them\n",
    "- **Resource Management**: Sophisticated scheduling across heterogeneous resources\n",
    "- **Scale**: Medium to very large installations\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Flexible resource allocation for different workload types\n",
    "- Can be built incrementally\n",
    "- Good balance of communication performance and scalability\n",
    "- Allows specialization of subsystems\n",
    "- Potentially good cost efficiency\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Complex programming model\n",
    "- Potentially unbalanced performance across subsystems\n",
    "- Challenging system management\n",
    "- Multiple failure modes to consider\n",
    "- Difficult performance optimization\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- NASA Columbia system (SGI Altix clusters)\n",
    "- Some configurations of Top500 systems\n",
    "- Academic HPC centers with connected specialized resources\n",
    "- National laboratory computing facilities\n",
    "- Industry research computing environments\n",
    "\n",
    "### Grid Computing\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Grid computing connects geographically distributed resources into a unified system:\n",
    "\n",
    "- Resources can be heterogeneous and administered separately\n",
    "- Wide-area networks connect components\n",
    "- Focus on resource sharing across organizational boundaries\n",
    "- Middleware for authentication, authorization, and resource discovery\n",
    "- Often used for loosely-coupled, high-throughput computing\n",
    "\n",
    "![Grid Computing Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Geographic Distribution**: Resources spread across locations\n",
    "- **Administrative Domains**: Multiple organizations controlling resources\n",
    "- **Connectivity**: Often standard internet protocols\n",
    "- **Resource Sharing**: Mechanisms for sharing across organizations\n",
    "- **Job Types**: Typically independent tasks (embarrassingly parallel)\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Utilization of otherwise idle resources\n",
    "- Access to specialized resources from multiple locations\n",
    "- Cost sharing across organizations\n",
    "- Potential for enormous scale\n",
    "- Support for virtual organizations\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Limited communication performance\n",
    "- Complex security requirements\n",
    "- Unreliable resource availability\n",
    "- Challenging application deployment\n",
    "- Uneven performance characteristics\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Open Science Grid\n",
    "- European Grid Infrastructure\n",
    "- SETI@home and similar volunteer computing projects\n",
    "- World Community Grid\n",
    "- TeraGrid (historical)\n",
    "\n",
    "## Specialized Architectures\n",
    "\n",
    "Specialized architectures are designed for specific types of computation, offering exceptional performance for targeted workloads but potentially limited applicability for general problems.\n",
    "\n",
    "### GPU Computing\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Graphics Processing Unit (GPU) computing leverages graphics hardware for general-purpose computing:\n",
    "\n",
    "- Massively parallel architecture with thousands of simple cores\n",
    "- Hierarchical organization of cores and memory\n",
    "- Specialized for single-instruction, multiple-thread (SIMT) execution\n",
    "- High memory bandwidth\n",
    "- Often used as accelerators alongside CPUs\n",
    "\n",
    "![GPU Computing Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Core Count**: Thousands of simple processing elements\n",
    "- **Memory Hierarchy**: Complex with global, shared, and private memories\n",
    "- **Programming Model**: Data-parallel with CUDA, OpenCL, or other frameworks\n",
    "- **Execution Model**: SIMT with warps/wavefronts\n",
    "- **Performance Profile**: Extremely high throughput for suitable problems\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Exceptional performance for parallel problems\n",
    "- High memory bandwidth\n",
    "- Good energy efficiency for suitable workloads\n",
    "- Continuous evolution driven by graphics market\n",
    "- Increasingly flexible programming models\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Programming complexity\n",
    "- Limited performance for serial code sections\n",
    "- Memory capacity constraints\n",
    "- Data transfer overheads between CPU and GPU\n",
    "- Not suitable for all algorithm types\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- NVIDIA Tesla, A100, H100 series\n",
    "- AMD Instinct series\n",
    "- Major supercomputers with GPU accelerators (Summit, Sierra, etc.)\n",
    "- Cloud GPU instances\n",
    "- Specialized AI/ML systems\n",
    "\n",
    "### FPGA-based Systems\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Field-Programmable Gate Array (FPGA) based systems offer reconfigurable hardware fabric:\n",
    "\n",
    "- Programmable logic blocks and interconnects\n",
    "- Custom datapaths tailored to specific algorithms\n",
    "- Potential for extreme energy efficiency\n",
    "- Direct implementation of algorithms in hardware\n",
    "- Often used as accelerators for specific functions\n",
    "\n",
    "![FPGA Architecture Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Reconfigurability**: Hardware can be reprogrammed for different functions\n",
    "- **Parallelism**: Spatial parallelism with custom datapaths\n",
    "- **Clock Rates**: Typically lower than CPUs/GPUs but with higher efficiency\n",
    "- **Programming Model**: Hardware description languages or high-level synthesis\n",
    "- **Integration**: Often as accelerator cards or embedded components\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Customizable hardware for algorithm requirements\n",
    "- Potential for excellent energy efficiency\n",
    "- Low latency for suitable applications\n",
    "- Deterministic performance\n",
    "- Adaptability to changing requirements\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Programming complexity\n",
    "- Long development cycles\n",
    "- Limited floating-point performance\n",
    "- Resource constraints\n",
    "- Challenging integration with traditional HPC software\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- Microsoft Catapult\n",
    "- Amazon EC2 F1 instances\n",
    "- Convey HC series\n",
    "- Maxeler dataflow engines\n",
    "- Specialized financial computing systems\n",
    "\n",
    "### Quantum Computing\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Quantum computers leverage quantum mechanical phenomena for computation:\n",
    "\n",
    "- Qubits instead of classical bits\n",
    "- Quantum superposition and entanglement\n",
    "- Quantum gates for operations\n",
    "- Specialized algorithms exploiting quantum properties\n",
    "- Currently in early developmental stages\n",
    "\n",
    "![Quantum Computing Conceptual Diagram]\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Qubits**: Fundamental quantum processing elements\n",
    "- **Coherence Time**: Duration qubits maintain quantum state\n",
    "- **Gate Fidelity**: Accuracy of quantum operations\n",
    "- **Error Correction**: Methods to mitigate quantum noise\n",
    "- **Programming Model**: Quantum circuits and algorithms\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Potential for exponential speedup for specific problems\n",
    "- Novel approach to currently intractable problems\n",
    "- Revolutionary potential for cryptography, simulation, and optimization\n",
    "- Active research area with rapid progress\n",
    "- Completely different computational paradigm\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Early technology with limited qubit counts\n",
    "- Short coherence times and high error rates\n",
    "- Requires extremely low temperatures (for most implementations)\n",
    "- Limited algorithmic applicability currently understood\n",
    "- Significant engineering challenges\n",
    "\n",
    "#### Implementation Examples\n",
    "\n",
    "- IBM Quantum systems\n",
    "- Google Sycamore\n",
    "- D-Wave quantum annealers\n",
    "- Honeywell/Quantinuum trapped ion systems\n",
    "- Rigetti superconducting qubit systems\n",
    "\n",
    "## Memory Hierarchies in HPC\n",
    "\n",
    "Memory hierarchies in HPC systems are critical for performance, as they bridge the gap between fast processors and slower storage systems.\n",
    "\n",
    "### Overview of Memory Hierarchies\n",
    "\n",
    "Modern HPC systems employ deep memory hierarchies:\n",
    "\n",
    "- Registers (fastest, smallest)\n",
    "- Multiple levels of cache (L1, L2, L3, sometimes L4)\n",
    "- Main memory (DRAM)\n",
    "- Non-volatile memory (NVRAM, persistent memory)\n",
    "- Solid-state storage\n",
    "- Hard disk storage\n",
    "- Archival storage (slowest, largest)\n",
    "\n",
    "### Characteristics Across Architectures\n",
    "\n",
    "Different HPC architectures implement memory hierarchies in various ways:\n",
    "\n",
    "#### SMP Systems\n",
    "- Uniform access to shared caches and memory\n",
    "- Cache coherence maintained across all processors\n",
    "- Relatively straightforward memory model\n",
    "\n",
    "#### NUMA Systems\n",
    "- Local and remote memory with different access characteristics\n",
    "- Potentially complex cache coherence protocols\n",
    "- Memory placement critical for performance\n",
    "\n",
    "#### Distributed Memory Systems\n",
    "- Private memory hierarchies for each node\n",
    "- Explicit data movement between nodes\n",
    "- Multiple levels of communication (intra-node, inter-node)\n",
    "\n",
    "#### Accelerator-Based Systems\n",
    "- Separate memory spaces for host and accelerator\n",
    "- Explicit data movement between host and accelerator\n",
    "- Specialized memory types (HBM, GDDR, etc.)\n",
    "\n",
    "### Impact on Performance\n",
    "\n",
    "Memory hierarchy characteristics dramatically impact HPC performance:\n",
    "\n",
    "- Memory latency often dominates runtime for many applications\n",
    "- Memory bandwidth limitations create bottlenecks\n",
    "- Data locality is crucial for performance optimization\n",
    "- Cache coherence overhead can limit scalability\n",
    "- Data movement energy costs often exceed computation energy\n",
    "\n",
    "### Recent Innovations\n",
    "\n",
    "Recent developments in HPC memory systems include:\n",
    "\n",
    "- High Bandwidth Memory (HBM)\n",
    "- Persistent memory technologies (Intel Optane, etc.)\n",
    "- Scratchpad memories for explicit management\n",
    "- Memory-side processing capabilities\n",
    "- Complex memory controller designs\n",
    "\n",
    "## Interconnect Technologies\n",
    "\n",
    "Interconnects are the communication fabric of HPC systems, enabling data exchange between nodes and components.\n",
    "\n",
    "### Key Interconnect Characteristics\n",
    "\n",
    "Important aspects of HPC interconnects include:\n",
    "\n",
    "- **Bandwidth**: Maximum data transfer rate\n",
    "- **Latency**: Delay in message transmission\n",
    "- **Topology**: Physical and logical arrangement of nodes\n",
    "- **Routing**: How messages find their paths\n",
    "- **Congestion Management**: Handling of traffic hotspots\n",
    "- **Reliability**: Error detection and correction capabilities\n",
    "- **Scalability**: Performance at different system sizes\n",
    "\n",
    "### Common Interconnect Technologies\n",
    "\n",
    "#### InfiniBand\n",
    "- Low latency, high bandwidth\n",
    "- Remote Direct Memory Access (RDMA) capabilities\n",
    "- Switched fabric architecture\n",
    "- Common in large HPC clusters\n",
    "\n",
    "#### High-Performance Ethernet\n",
    "- Enhanced Ethernet with DCB, RoCE\n",
    "- Widely available, cost-effective\n",
    "- Growing capabilities for HPC workloads\n",
    "- Common in smaller clusters\n",
    "\n",
    "#### Proprietary Interconnects\n",
    "- Cray Slingshot\n",
    "- Intel Omni-Path\n",
    "- Fujitsu Tofu\n",
    "- Optimized for specific system architectures\n",
    "\n",
    "#### On-chip Interconnects\n",
    "- Network-on-Chip (NoC) designs\n",
    "- Crucial for many-core processors\n",
    "- Determining factor in NUMA performance\n",
    "\n",
    "### Network Topologies\n",
    "\n",
    "The arrangement of nodes and switches significantly impacts system performance:\n",
    "\n",
    "#### Mesh and Torus\n",
    "- Regular structure, good for nearest-neighbor communication\n",
    "- Used in many supercomputers (Blue Gene, K computer)\n",
    "- Relatively simple routing\n",
    "\n",
    "#### Fat Tree\n",
    "- Non-blocking at full bisection bandwidth\n",
    "- Common in InfiniBand clusters\n",
    "- Good general-purpose topology\n",
    "\n",
    "#### Dragonfly\n",
    "- High-radix routers with group structure\n",
    "- Used in Cray systems\n",
    "- Good balance of performance and cost\n",
    "\n",
    "#### Hypercube\n",
    "- Logarithmic diameter with node count\n",
    "- Historical importance (Connection Machine)\n",
    "- Complex physical implementation\n",
    "\n",
    "## Performance Metrics and Benchmarking\n",
    "\n",
    "Understanding HPC system performance requires specialized metrics and benchmarking approaches.\n",
    "\n",
    "### Key Performance Metrics\n",
    "\n",
    "Important metrics for evaluating HPC systems include:\n",
    "\n",
    "- **FLOPS (Floating-Point Operations Per Second)**: Raw computational capability\n",
    "- **Memory Bandwidth**: Data transfer rate to/from memory\n",
    "- **Memory Latency**: Time to access data from memory\n",
    "- **Network Bandwidth**: Maximum data transfer rate between nodes\n",
    "- **Network Latency**: Delay in message transmission\n",
    "- **I/O Bandwidth**: Data transfer rate to/from storage\n",
    "- **Energy Efficiency**: Performance per watt of power\n",
    "- **Reliability**: Mean time between failures\n",
    "\n",
    "### Benchmark Suites\n",
    "\n",
    "Standard benchmark suites provide comparative performance data:\n",
    "\n",
    "#### LINPACK\n",
    "- Solves dense linear equations\n",
    "- Used for TOP500 ranking\n",
    "- Limited representation of real applications\n",
    "\n",
    "#### HPCG (High Performance Conjugate Gradients)\n",
    "- Sparse linear algebra focus\n",
    "- More representative of many applications\n",
    "- Complements LINPACK for system evaluation\n",
    "\n",
    "#### SPEC HPC Benchmarks\n",
    "- Suite of application benchmarks\n",
    "- Covers multiple application domains\n",
    "- More realistic workload representation\n",
    "\n",
    "#### Application-Specific Benchmarks\n",
    "- Domain-specific performance evaluation\n",
    "- Often most relevant to actual usage patterns\n",
    "- Less useful for cross-system comparison\n",
    "\n",
    "### Performance Analysis Tools\n",
    "\n",
    "Tools for understanding HPC performance include:\n",
    "\n",
    "- **Profilers**: Detailed timing information\n",
    "- **Tracers**: Event sequence recording\n",
    "- **Hardware Counters**: Low-level performance events\n",
    "- **Network Analyzers**: Communication pattern analysis\n",
    "- **I/O Profilers**: Storage access patterns\n",
    "- **Energy Monitors**: Power consumption tracking\n",
    "\n",
    "## Programming Models for HPC\n",
    "\n",
    "Programming models provide abstractions for expressing parallel algorithms on HPC architectures.\n",
    "\n",
    "### Shared Memory Programming\n",
    "\n",
    "#### OpenMP\n",
    "- Directive-based parallelism\n",
    "- Incremental parallelization approach\n",
    "- Well-suited for SMP and NUMA systems\n",
    "- Relatively easy to learn and apply\n",
    "\n",
    "#### POSIX Threads\n",
    "- Lower-level threading model\n",
    "- Fine-grained control\n",
    "- Basis for many other threading models\n",
    "- More complex programming model\n",
    "\n",
    "#### Threading Building Blocks (TBB)\n",
    "- C++ template library for parallelism\n",
    "- Task-based programming model\n",
    "- Dynamic load balancing\n",
    "- Good performance on multi-core systems\n",
    "\n",
    "### Distributed Memory Programming\n",
    "\n",
    "#### Message Passing Interface (MPI)\n",
    "- De facto standard for distributed memory programming\n",
    "- Explicit message passing\n",
    "- Extensive functionality\n",
    "- Scales to largest systems\n",
    "- Relatively complex programming model\n",
    "\n",
    "#### Partitioned Global Address Space (PGAS)\n",
    "- Global view of address space with locality awareness\n",
    "- Languages and libraries including UPC, Chapel, X10\n",
    "- Balances programmer productivity and performance\n",
    "- Still gaining adoption in HPC community\n",
    "\n",
    "### Heterogeneous Computing\n",
    "\n",
    "#### CUDA\n",
    "- NVIDIA's platform for GPU computing\n",
    "- C/C++ language extensions\n",
    "- Extensive ecosystem\n",
    "- Vendor-specific\n",
    "\n",
    "#### OpenCL\n",
    "- Open standard for heterogeneous computing\n",
    "- Supports multiple device types\n",
    "- Cross-vendor support\n",
    "- More complex than vendor-specific options\n",
    "\n",
    "#### OpenACC\n",
    "- Directive-based acceleration\n",
    "- Similar approach to OpenMP\n",
    "- Focus on incremental optimization\n",
    "- Supports multiple accelerator types\n",
    "\n",
    "### Domain-Specific Languages (DSLs)\n",
    "\n",
    "- **Specialized syntax** for particular application domains\n",
    "- **Higher-level abstractions** than general-purpose languages\n",
    "- **Performance optimizations** specific to domain characteristics\n",
    "- **Examples**: TensorFlow (ML), Halide (image processing), RAJA (DOE applications)\n",
    "\n",
    "## Current Trends and Future Directions\n",
    "\n",
    "HPC architecture continues to evolve rapidly, driven by technological innovations and changing computational needs.\n",
    "\n",
    "### Exascale Computing\n",
    "\n",
    "The push for systems capable of 10^18 floating-point operations per second:\n",
    "\n",
    "- Enormous parallelism (millions of cores)\n",
    "- Power efficiency as primary design constraint\n",
    "- Resilience against frequent component failures\n",
    "- Novel programming models for extreme concurrency\n",
    "- First systems deployed at national laboratories\n",
    "\n",
    "### Convergence of HPC and AI\n",
    "\n",
    "The increasing overlap between traditional HPC and artificial intelligence:\n",
    "\n",
    "- Specialized hardware for AI within HPC systems\n",
    "- Hybrid workflows combining simulation and ML\n",
    "- New programming models spanning both domains\n",
    "- Systems designed for both floating-point and integer/reduced precision\n",
    "- Data-centric architectures\n",
    "\n",
    "### Heterogeneous Computing\n",
    "\n",
    "The growing diversity of computing elements within systems:\n",
    "\n",
    "- Multiple processor types within a single system\n",
    "- Specialized accelerators for particular operations\n",
    "- Reconfigurable components for adaptability\n",
    "- Complex memory hierarchies\n",
    "- Increasingly sophisticated programming models\n",
    "\n",
    "### Near-Memory and In-Memory Computing\n",
    "\n",
    "Moving computation closer to data:\n",
    "\n",
    "- Processing elements integrated with memory\n",
    "- Reduced data movement energy and latency\n",
    "- New programming abstractions for memory-centric computing\n",
    "- Potential architectural revolution\n",
    "- Addressing the memory wall problem\n",
    "\n",
    "### Quantum and Neuromorphic Computing\n",
    "\n",
    "Alternative computing paradigms:\n",
    "\n",
    "- Quantum computers for specific problem classes\n",
    "- Neuromorphic systems inspired by biological neural networks\n",
    "- Hybrid systems combining traditional and novel approaches\n",
    "- Specialized programming models\n",
    "- Potential for dramatic performance improvements in specific domains\n",
    "\n",
    "## Glossary of HPC Terms\n",
    "\n",
    "### A-D\n",
    "\n",
    "- **Accelerator**: Specialized hardware component designed to speed up specific computations\n",
    "- **Amdahl's Law**: Formula showing the theoretical speedup limited by serial portions of code\n",
    "- **Bandwidth**: Rate at which data can be transferred\n",
    "- **Bisection Bandwidth**: Worst-case bandwidth when a system is divided into two equal parts\n",
    "- **Cache Coherence**: Maintaining consistent cache data across multiple caches\n",
    "- **Cluster**: Collection of interconnected computers working as a unified system\n",
    "- **Core**: Individual processing unit within a processor\n",
    "- **Distributed Memory**: Memory architecture where each processor has its own private memory\n",
    "\n",
    "### E-H\n",
    "\n",
    "- **Embarrassingly Parallel**: Problems easily divided into independent parallel tasks\n",
    "- **Exascale**: Computing systems capable of at least 10^18 floating-point operations per second\n",
    "- **Fabric**: The communication infrastructure connecting components\n",
    "- **FLOPS**: Floating-Point Operations Per Second, measure of computer performance\n",
    "- **GPU**: Graphics Processing Unit, used for parallel computation\n",
    "- **HBM**: High Bandwidth Memory, stacked memory technology\n",
    "- **Heterogeneous Computing**: Using multiple types of processors or cores together\n",
    "- **HPC**: High-Performance Computing\n",
    "\n",
    "### I-L\n",
    "\n",
    "- **Infiniband**: High-performance network technology common in HPC\n",
    "- **Interconnect**: Network connecting components of an HPC system\n",
    "- **Latency**: Time delay between initiation and execution of an operation\n",
    "- **Load Balancing**: Distributing workload evenly across computing resources\n",
    "\n",
    "### M-P\n",
    "\n",
    "- **MPI**: Message Passing Interface, standard for distributed memory programming\n",
    "- **Node**: Individual computer in a cluster or supercomputer\n",
    "- **NUMA**: Non-Uniform Memory Access architecture\n",
    "- **OpenMP**: API for shared-memory multiprocessing\n",
    "- **Parallel Efficiency**: Measure of how well additional resources are utilized\n",
    "- **Petascale**: Computing systems capable of at least 10^15 FLOPS\n",
    "- **PGAS**: Partitioned Global Address Space programming model\n",
    "\n",
    "### Q-T\n",
    "\n",
    "- **QPI**: QuickPath Interconnect, Intel's processor interconnect technology\n",
    "- **RDMA**: Remote Direct Memory Access, allows direct access to memory across nodes\n",
    "- **Scalability**: Ability of a system to handle growing amounts of work\n",
    "- **SIMD**: Single Instruction, Multiple Data parallelism\n",
    "- **SMP**: Symmetric Multiprocessing architecture\n",
    "- **Strong Scaling**: How performance varies with processor count at fixed problem size\n",
    "- **Throughput**: Amount of work done per unit time\n",
    "- **TOP500**: List of the 500 most powerful supercomputer systems\n",
    "\n",
    "### U-Z\n",
    "\n",
    "- **UMA**: Uniform Memory Access architecture\n",
    "- **Vector Processing**: Processing applied to multiple data elements simultaneously\n",
    "- **Weak Scaling**: How performance varies with processor count at fixed problem size per processor\n",
    "- **Xeon**: Intel's processor line often used in HPC systems\n",
    "- **Xeon Phi**: Intel's (now discontinued) many-core processor architecture\n",
    "- **ZettaFLOPS**: 10^21 floating-point operations per second\n",
    "\n",
    "---\n",
    "\n",
    "*This document provides a comprehensive overview of HPC architectures, but the field is constantly evolving. For the most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
