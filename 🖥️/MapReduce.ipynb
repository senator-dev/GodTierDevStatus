{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce: A Comprehensive Overview\n",
    "\n",
    "## Introduction to MapReduce\n",
    "\n",
    "MapReduce is a revolutionary programming model and processing technique designed for distributed computing, primarily developed by Google to process and generate large datasets across clusters of commodity hardware. It represents a paradigm shift in handling big data by breaking down complex computational tasks into parallelizable units that can be processed independently and then aggregated.\n",
    "\n",
    "## Core Principles\n",
    "\n",
    "### 1. Fundamental Concept\n",
    "MapReduce operates on two primary functions:\n",
    "- **Map Function**: Transforms input data into key-value pairs\n",
    "- **Reduce Function**: Aggregates and summarizes the mapped data\n",
    "\n",
    "### 2. Execution Flow\n",
    "The typical MapReduce workflow consists of several key stages:\n",
    "1. Input Data Splitting\n",
    "2. Mapping\n",
    "3. Shuffling and Sorting\n",
    "4. Reducing\n",
    "5. Final Output Generation\n",
    "\n",
    "## Detailed Architecture\n",
    "\n",
    "### Input Data Partitioning\n",
    "- Large input datasets are automatically divided into fixed-size chunks (typically 64-128 MB)\n",
    "- Each chunk is processed independently across multiple machines in a cluster\n",
    "- Enables horizontal scalability and parallel processing\n",
    "\n",
    "### Map Phase\n",
    "- Receives input as key-value pairs\n",
    "- Processes each input record independently\n",
    "- Generates intermediate key-value pairs\n",
    "- Completely parallelizable across multiple machines\n",
    "- Transforms raw data into a structured format suitable for aggregation\n",
    "\n",
    "### Shuffle and Sort Phase\n",
    "- Intermediate phase between Map and Reduce\n",
    "- Redistributes mapped data across reducer nodes\n",
    "- Groups all values associated with the same key\n",
    "- Handles data movement and organization\n",
    "- Critical for preparing data for reduction\n",
    "\n",
    "### Reduce Phase\n",
    "- Receives grouped key-value pairs from the shuffle phase\n",
    "- Performs aggregation, summarization, or complex computations\n",
    "- Generates final output records\n",
    "- Consolidates results from multiple mappers\n",
    "\n",
    "## Practical Example: Word Count\n",
    "\n",
    "```python\n",
    "def map_function(document):\n",
    "    for word in document.split():\n",
    "        yield (word, 1)\n",
    "\n",
    "def reduce_function(word, counts):\n",
    "    return (word, sum(counts))\n",
    "```\n",
    "\n",
    "## Advantages of MapReduce\n",
    "\n",
    "1. **Scalability**\n",
    "   - Easily scales horizontally by adding more machines\n",
    "   - Handles petabyte-scale datasets\n",
    "   - Automatic parallelization of computational tasks\n",
    "\n",
    "2. **Fault Tolerance**\n",
    "   - Automatically handles machine failures\n",
    "   - Redistributes tasks if a node goes down\n",
    "   - Ensures job completion despite hardware issues\n",
    "\n",
    "3. **Simplicity**\n",
    "   - Abstracts complex distributed computing challenges\n",
    "   - Developers focus on mapping and reducing logic\n",
    "   - Framework handles infrastructure complexities\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. High latency for small computations\n",
    "2. Not ideal for iterative or real-time processing\n",
    "3. Overhead in task scheduling and data movement\n",
    "4. Limited support for complex algorithmic patterns\n",
    "\n",
    "## Implementation Frameworks\n",
    "\n",
    "1. **Hadoop MapReduce**\n",
    "   - Open-source implementation\n",
    "   - Most widely used MapReduce framework\n",
    "   - Part of Apache Hadoop ecosystem\n",
    "\n",
    "2. **Google MapReduce**\n",
    "   - Original proprietary implementation\n",
    "   - Inspired many subsequent distributed computing models\n",
    "\n",
    "3. **Apache Spark**\n",
    "   - Modern evolution of MapReduce\n",
    "   - Supports in-memory computing\n",
    "   - More flexible processing model\n",
    "\n",
    "## Modern Relevance\n",
    "\n",
    "While newer technologies like Apache Spark have emerged, MapReduce remains foundational in understanding distributed computing principles. Its core concepts continue to influence big data processing frameworks worldwide.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "MapReduce represents a paradigmatic approach to solving large-scale computational problems by leveraging distributed computing's power. Its elegance lies in transforming complex problems into simple, parallelizable computational units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Hadoop: A Comprehensive Exploration\n",
    "\n",
    "## Introduction to Apache Hadoop\n",
    "\n",
    "Apache Hadoop is an open-source framework designed for distributed storage and processing of large-scale datasets across clusters of commodity hardware. Developed by the Apache Software Foundation, Hadoop revolutionized big data processing by providing a scalable, fault-tolerant, and cost-effective solution for handling massive amounts of data.\n",
    "\n",
    "## Historical Context\n",
    "\n",
    "### Origins\n",
    "- Created by Doug Cutting and Mike Cafarella in 2006\n",
    "- Inspired by Google's published papers on distributed computing\n",
    "- Named after Doug Cutting's son's toy elephant\n",
    "- Became an Apache top-level project in 2009\n",
    "\n",
    "## Core Components of Hadoop Ecosystem\n",
    "\n",
    "### 1. Hadoop Distributed File System (HDFS)\n",
    "#### Architecture\n",
    "- Distributed, scalable, and portable file system\n",
    "- Designed to store very large files across multiple machines\n",
    "- Provides high aggregate bandwidth and fault tolerance\n",
    "\n",
    "#### Key Characteristics\n",
    "- Master-Slave Architecture\n",
    "  - NameNode (Master): Manages file system metadata\n",
    "  - DataNodes (Slaves): Store actual data blocks\n",
    "- Data Replication\n",
    "  - Typically maintains 3 copies of each data block\n",
    "  - Ensures high availability and fault tolerance\n",
    "- Block-based Storage\n",
    "  - Default block size: 128 MB or 256 MB\n",
    "  - Enables efficient data distribution and parallel processing\n",
    "\n",
    "### 2. MapReduce\n",
    "#### Relationship to Hadoop\n",
    "- Originally the primary processing framework in Hadoop\n",
    "- Provides distributed processing capabilities\n",
    "- Enables parallel computation across the cluster\n",
    "\n",
    "#### Execution Model\n",
    "- Map Phase: Transforms input data into key-value pairs\n",
    "- Reduce Phase: Aggregates and summarizes mapped data\n",
    "- Handles complex distributed computing tasks\n",
    "\n",
    "### 3. YARN (Yet Another Resource Negotiator)\n",
    "#### Purpose\n",
    "- Resource management layer\n",
    "- Job scheduling and cluster resource allocation\n",
    "- Introduced in Hadoop 2.0\n",
    "\n",
    "#### Key Features\n",
    "- Supports multiple processing engines beyond MapReduce\n",
    "- Allows concurrent execution of different workloads\n",
    "- Improves cluster utilization and flexibility\n",
    "\n",
    "## Hadoop Ecosystem Components\n",
    "\n",
    "### 1. Data Storage and Ingestion\n",
    "- **HBase**: Column-oriented NoSQL database\n",
    "- **Hive**: Data warehousing and SQL-like querying\n",
    "- **Pig**: High-level data flow scripting language\n",
    "- **Sqoop**: Data transfer between Hadoop and relational databases\n",
    "\n",
    "### 2. Data Processing\n",
    "- **Spark**: In-memory distributed processing\n",
    "- **Flink**: Stream and batch data processing\n",
    "- **Storm**: Real-time stream processing\n",
    "\n",
    "### 3. Coordination and Management\n",
    "- **ZooKeeper**: Distributed coordination service\n",
    "- **Oozie**: Workflow and coordination system\n",
    "\n",
    "## Detailed Architecture\n",
    "\n",
    "### Cluster Configuration\n",
    "1. **Master Nodes**\n",
    "   - NameNode (HDFS)\n",
    "   - ResourceManager (YARN)\n",
    "   - Secondary NameNode\n",
    "\n",
    "2. **Worker Nodes**\n",
    "   - DataNodes\n",
    "   - NodeManagers\n",
    "   - Task Trackers\n",
    "\n",
    "### Data Storage Mechanism\n",
    "- Files split into fixed-size blocks\n",
    "- Blocks distributed across cluster\n",
    "- Replication for fault tolerance\n",
    "- Metadata managed by NameNode\n",
    "\n",
    "## Performance Characteristics\n",
    "\n",
    "### Strengths\n",
    "1. Horizontal Scalability\n",
    "   - Add commodity hardware to increase capacity\n",
    "   - Linear performance scaling\n",
    "2. Fault Tolerance\n",
    "   - Automatic data replication\n",
    "   - Node failure handling\n",
    "3. Cost-Effectiveness\n",
    "   - Runs on inexpensive commodity hardware\n",
    "   - Open-source software\n",
    "\n",
    "### Limitations\n",
    "1. High latency for small computations\n",
    "2. Not ideal for real-time processing\n",
    "3. Complex setup and maintenance\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "1. **Web Log Analysis**\n",
    "2. **Scientific Data Processing**\n",
    "3. **Machine Learning**\n",
    "4. **Financial Risk Modeling**\n",
    "5. **Recommendation Systems**\n",
    "\n",
    "## Evolution and Modern Trends\n",
    "\n",
    "### Hadoop 3.x Innovations\n",
    "- Erasure Coding (reduced storage overhead)\n",
    "- Docker and Kubernetes support\n",
    "- Improved resource management\n",
    "- Enhanced security features\n",
    "\n",
    "### Future Directions\n",
    "- Increased cloud integration\n",
    "- Better support for machine learning\n",
    "- Improved real-time processing capabilities\n",
    "\n",
    "## Code Example: Simple Hadoop MapReduce Job (Java)\n",
    "\n",
    "```java\n",
    "public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String[] words = value.toString().split(\"\\\\s+\");\n",
    "        for (String word : words) {\n",
    "            context.write(new Text(word), new IntWritable(1));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterable<IntWritable> values, Context context) \n",
    "        throws IOException, InterruptedException {\n",
    "        int sum = 0;\n",
    "        for (IntWritable val : values) {\n",
    "            sum += val.get();\n",
    "        }\n",
    "        context.write(key, new IntWritable(sum));\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Apache Hadoop represents a transformative approach to big data processing, providing a robust, scalable, and flexible framework for handling massive datasets. Its ecosystem continues to evolve, adapting to emerging computational challenges and technological advancements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Internals: A Comprehensive Technical Exploration\n",
    "\n",
    "## 1. HDFS Architecture: Deep Technical Breakdown\n",
    "\n",
    "### NameNode Internals\n",
    "#### Metadata Management\n",
    "- Maintains two critical files:\n",
    "  1. **FsImage**: Filesystem namespace snapshot\n",
    "  2. **EditLog**: Transaction log of filesystem changes\n",
    "\n",
    "#### Metadata Storage Mechanisms\n",
    "- Stored in memory for rapid access\n",
    "- Periodically checkpointed to persistent storage\n",
    "- Supports namespace operations:\n",
    "  - File creation\n",
    "  - Directory management\n",
    "  - Permission tracking\n",
    "  - Block location mapping\n",
    "\n",
    "### DataNode Operational Details\n",
    "#### Block Management\n",
    "- Sends periodic heartbeats to NameNode\n",
    "- Reports block list every 3.5 minutes\n",
    "- Supports block operations:\n",
    "  - Creation\n",
    "  - Deletion\n",
    "  - Replication\n",
    "  - Verification\n",
    "\n",
    "#### Data Transfer Protocols\n",
    "- Uses TCP/IP for block transfer\n",
    "- Supports pipelined data writing\n",
    "- Implements checksum verification\n",
    "- Supports concurrent read/write operations\n",
    "\n",
    "## 2. Advanced MapReduce Execution Flow\n",
    "\n",
    "### Detailed Execution Phases\n",
    "1. **Input Split Generation**\n",
    "   - Divides input data into logical splits\n",
    "   - Typically 128MB-256MB per split\n",
    "   - Maximizes data locality\n",
    "\n",
    "2. **Map Phase Execution**\n",
    "   - Input records parsed independently\n",
    "   - Key-value pair generation\n",
    "   - Local in-memory buffering\n",
    "   - Periodic spilling to disk\n",
    "\n",
    "3. **Shuffle and Sort**\n",
    "   - Network-intensive operation\n",
    "   - Merges mapped data\n",
    "   - Sorts by key\n",
    "   - Handles data partitioning\n",
    "\n",
    "4. **Reduce Phase**\n",
    "   - Aggregates mapped data\n",
    "   - Performs complex transformations\n",
    "   - Generates final output\n",
    "\n",
    "## 3. Memory Management Strategies\n",
    "\n",
    "### Map-Side Memory Management\n",
    "- Uses circular memory buffers\n",
    "- Configurable buffer sizes\n",
    "- Automatic spilling mechanism\n",
    "- Supports compression techniques\n",
    "\n",
    "### Reduce-Side Memory Management\n",
    "- Merge-sort based memory allocation\n",
    "- Handles large-scale data aggregation\n",
    "- Supports multiple memory optimization techniques\n",
    "\n",
    "## 4. Advanced Scheduling Mechanisms\n",
    "\n",
    "### YARN Resource Allocation\n",
    "#### Resource Negotiation\n",
    "- Dynamic resource allocation\n",
    "- Container-based execution model\n",
    "- Supports multiple workload types\n",
    "\n",
    "#### Scheduling Algorithms\n",
    "1. **Capacity Scheduler**\n",
    "   - Guarantees minimum resource allocation\n",
    "   - Supports multi-tenant environments\n",
    "\n",
    "2. **Fair Scheduler**\n",
    "   - Dynamically shares cluster resources\n",
    "   - Ensures job fairness\n",
    "   - Supports priority-based allocation\n",
    "\n",
    "## 5. Low-Level Performance Optimization\n",
    "\n",
    "### Data Locality Optimization\n",
    "- Moves computation to data\n",
    "- Minimizes network transfer overhead\n",
    "- Intelligent block placement strategies\n",
    "\n",
    "### Network Optimization\n",
    "- Rack-aware block placement\n",
    "- Minimizes inter-rack data transfer\n",
    "- Improves overall cluster efficiency\n",
    "\n",
    "## 6. Advanced Error Handling\n",
    "\n",
    "### Failure Detection Mechanisms\n",
    "- Heartbeat-based node monitoring\n",
    "- Automatic task reallocation\n",
    "- Block re-replication strategies\n",
    "- Supports various failure scenarios\n",
    "\n",
    "### Data Integrity Verification\n",
    "- Checksum-based block verification\n",
    "- Automatic corruption detection\n",
    "- Self-healing capabilities\n",
    "\n",
    "## 7. Security Enhancements\n",
    "\n",
    "### Authentication Mechanisms\n",
    "- Kerberos integration\n",
    "- Service-level authentication\n",
    "- Supports:\n",
    "  - User authentication\n",
    "  - Service-to-service encryption\n",
    "  - Role-based access control\n",
    "\n",
    "### Data Encryption\n",
    "- Transparent encryption\n",
    "- Key management services\n",
    "- Support for:\n",
    "  - Data-at-rest encryption\n",
    "  - Network-level encryption\n",
    "\n",
    "## 8. Performance Tuning Parameters\n",
    "\n",
    "### Critical Configuration Options\n",
    "- `dfs.blocksize`\n",
    "- `mapreduce.job.reduces`\n",
    "- `yarn.nodemanager.resource.memory-mb`\n",
    "- `mapreduce.map.memory.mb`\n",
    "- `mapreduce.reduce.memory.mb`\n",
    "\n",
    "### Optimization Strategies\n",
    "- Vertical scaling\n",
    "- Horizontal scaling\n",
    "- Intelligent resource allocation\n",
    "- Workload-specific tuning\n",
    "\n",
    "## 9. Emerging Trends\n",
    "\n",
    "### Modern Hadoop Adaptations\n",
    "- Cloud-native architectures\n",
    "- Kubernetes integration\n",
    "- Serverless computing models\n",
    "- Machine learning accelerations\n",
    "\n",
    "## Code Example: Advanced Configuration\n",
    "\n",
    "```java\n",
    "Configuration conf = new Configuration();\n",
    "conf.set(\"dfs.replication\", \"3\");\n",
    "conf.set(\"mapreduce.job.reduces\", \"10\");\n",
    "conf.setInt(\"mapreduce.map.memory.mb\", 4096);\n",
    "conf.setInt(\"mapreduce.reduce.memory.mb\", 8192);\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Hadoop represents a sophisticated distributed computing ecosystem with intricate internal mechanisms designed to handle massive-scale data processing efficiently, reliably, and flexibly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Hadoop Task Examples\n",
    "\n",
    "## 1. Word Count: Classic Big Data Problem\n",
    "\n",
    "```java\n",
    "public class WordCountDriver extends Configured implements Tool {\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        int res = ToolRunner.run(new Configuration(), new WordCountDriver(), args);\n",
    "        System.exit(res);\n",
    "    }\n",
    "\n",
    "    public int run(String[] args) throws Exception {\n",
    "        Job job = Job.getInstance(getConf(), \"WordCount\");\n",
    "        job.setJarByClass(WordCountDriver.class);\n",
    "        \n",
    "        // Input and Output Paths\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        \n",
    "        // Mapper and Reducer Classes\n",
    "        job.setMapperClass(TokenizerMapper.class);\n",
    "        job.setReducerClass(IntSumReducer.class);\n",
    "        \n",
    "        // Output Types\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "        \n",
    "        return job.waitForCompletion(true) ? 0 : 1;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 2. Log Analysis: Processing Web Server Logs\n",
    "\n",
    "```java\n",
    "public class LogAnalysisMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private static final IntWritable one = new IntWritable(1);\n",
    "    private Text errorType = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String[] logFields = value.toString().split(\" \");\n",
    "        \n",
    "        // Extract HTTP status code\n",
    "        if (logFields.length > 8) {\n",
    "            String statusCode = logFields[8];\n",
    "            \n",
    "            // Categorize error types\n",
    "            if (statusCode.startsWith(\"4\") || statusCode.startsWith(\"5\")) {\n",
    "                errorType.set(\"ERROR_\" + statusCode.charAt(0) + \"XX\");\n",
    "                context.write(errorType, one);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 3. Sales Data Aggregation\n",
    "\n",
    "```java\n",
    "public class SalesAggregationMapper extends Mapper<LongWritable, Text, Text, DoubleWritable> {\n",
    "    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String[] fields = value.toString().split(\",\");\n",
    "        // Assuming format: date,product,quantity,price\n",
    "        String product = fields[1];\n",
    "        double revenue = Double.parseDouble(fields[2]) * Double.parseDouble(fields[3]);\n",
    "        \n",
    "        context.write(new Text(product), new DoubleWritable(revenue));\n",
    "    }\n",
    "}\n",
    "\n",
    "public class SalesAggregationReducer extends Reducer<Text, DoubleWritable, Text, DoubleWritable> {\n",
    "    public void reduce(Text key, Iterable<DoubleWritable> values, Context context) \n",
    "            throws IOException, InterruptedException {\n",
    "        double totalRevenue = 0.0;\n",
    "        for (DoubleWritable value : values) {\n",
    "            totalRevenue += value.get();\n",
    "        }\n",
    "        context.write(key, new DoubleWritable(totalRevenue));\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 4. User Behavior Analysis\n",
    "\n",
    "```java\n",
    "public class UserBehaviorMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String[] fields = value.toString().split(\"\\t\");\n",
    "        // Assuming log format: user_id, action_type, timestamp\n",
    "        String userId = fields[0];\n",
    "        String actionType = fields[1];\n",
    "        \n",
    "        context.write(new Text(userId + \"_\" + actionType), new IntWritable(1));\n",
    "    }\n",
    "}\n",
    "\n",
    "public class UserBehaviorReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterable<IntWritable> values, Context context) \n",
    "            throws IOException, InterruptedException {\n",
    "        int actionCount = 0;\n",
    "        for (IntWritable value : values) {\n",
    "            actionCount += value.get();\n",
    "        }\n",
    "        context.write(key, new IntWritable(actionCount));\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 5. Sensor Data Processing\n",
    "\n",
    "```java\n",
    "public class SensorDataMapper extends Mapper<LongWritable, Text, Text, DoubleWritable> {\n",
    "    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String[] readings = value.toString().split(\",\");\n",
    "        // Assuming format: sensor_id, timestamp, temperature, humidity\n",
    "        String sensorId = readings[0];\n",
    "        double temperature = Double.parseDouble(readings[2]);\n",
    "        \n",
    "        // Filter for extreme temperatures\n",
    "        if (temperature > 40.0 || temperature < -10.0) {\n",
    "            context.write(new Text(sensorId + \"_EXTREME_TEMP\"), new DoubleWritable(temperature));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 6. Network Traffic Analysis\n",
    "\n",
    "```java\n",
    "public class NetworkTrafficMapper extends Mapper<LongWritable, Text, Text, LongWritable> {\n",
    "    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String[] fields = value.toString().split(\",\");\n",
    "        // Assuming format: source_ip, destination_ip, bytes_transferred, timestamp\n",
    "        String sourceIP = fields[0];\n",
    "        long byteTransferred = Long.parseLong(fields[2]);\n",
    "        \n",
    "        context.write(new Text(sourceIP), new LongWritable(byteTransferred));\n",
    "    }\n",
    "}\n",
    "\n",
    "public class NetworkTrafficReducer extends Reducer<Text, LongWritable, Text, LongWritable> {\n",
    "    public void reduce(Text key, Iterable<LongWritable> values, Context context) \n",
    "            throws IOException, InterruptedException {\n",
    "        long totalTraffic = 0;\n",
    "        for (LongWritable value : values) {\n",
    "            totalTraffic += value.get();\n",
    "        }\n",
    "        context.write(key, new LongWritable(totalTraffic));\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 7. Social Media Sentiment Analysis\n",
    "\n",
    "```java\n",
    "public class SentimentAnalysisMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private static final IntWritable positive = new IntWritable(1);\n",
    "    private static final IntWritable negative = new IntWritable(-1);\n",
    "    private static final IntWritable neutral = new IntWritable(0);\n",
    "\n",
    "    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String tweet = value.toString().toLowerCase();\n",
    "        \n",
    "        // Simple sentiment analysis\n",
    "        if (tweet.contains(\"great\") || tweet.contains(\"awesome\") || tweet.contains(\"excellent\")) {\n",
    "            context.write(new Text(\"POSITIVE_SENTIMENT\"), positive);\n",
    "        } else if (tweet.contains(\"bad\") || tweet.contains(\"terrible\") || tweet.contains(\"awful\")) {\n",
    "            context.write(new Text(\"NEGATIVE_SENTIMENT\"), negative);\n",
    "        } else {\n",
    "            context.write(new Text(\"NEUTRAL_SENTIMENT\"), neutral);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 8. E-commerce Product Recommendation\n",
    "\n",
    "```java\n",
    "public class ProductRecommendationMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String[] purchaseRecord = value.toString().split(\",\");\n",
    "        // Assuming format: user_id, product_id, purchase_timestamp\n",
    "        String userId = purchaseRecord[0];\n",
    "        String productId = purchaseRecord[1];\n",
    "        \n",
    "        context.write(new Text(userId + \"_PURCHASED\"), new IntWritable(Integer.parseInt(productId)));\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "These examples demonstrate the versatility of Hadoop MapReduce in handling various data processing scenarios across different domains, from log analysis to complex data aggregation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop MapReduce Across Different Programming Languages\n",
    "\n",
    "## 1. Python (Using Hadoop Streaming)\n",
    "\n",
    "### Word Count Example\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "# Mapper\n",
    "def mapper():\n",
    "    for line in sys.stdin:\n",
    "        words = line.strip().lower().split()\n",
    "        for word in words:\n",
    "            print(f\"{word}\\t1\")\n",
    "\n",
    "# Reducer\n",
    "def reducer():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split('\\t')\n",
    "        count = int(count)\n",
    "\n",
    "        if current_word == word:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word:\n",
    "                print(f\"{current_word}\\t{current_count}\")\n",
    "            current_word = word\n",
    "            current_count = count\n",
    "\n",
    "    if current_word:\n",
    "        print(f\"{current_word}\\t{current_count}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    mapper() if 'map' in sys.argv[0] else reducer()\n",
    "```\n",
    "\n",
    "## 2. Scala (Using Apache Spark with Hadoop)\n",
    "```scala\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "object WordCount {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = new SparkConf()\n",
    "      .setAppName(\"Word Count\")\n",
    "      .setMaster(\"yarn\")\n",
    "\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    val textFile = sc.textFile(\"hdfs://input/path\")\n",
    "    val wordCounts = textFile\n",
    "      .flatMap(line => line.toLowerCase.split(\"\\\\s+\"))\n",
    "      .map(word => (word, 1))\n",
    "      .reduceByKey(_ + _)\n",
    "\n",
    "    wordCounts.saveAsTextFile(\"hdfs://output/path\")\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 3. R (Using RHadoop)\n",
    "```r\n",
    "library(rmr2)\n",
    "library(rhdfs)\n",
    "\n",
    "# Mapper function\n",
    "word.count.map <- function(input) {\n",
    "  words <- strsplit(tolower(input), \"\\\\s+\")[[1]]\n",
    "  keyval(words, 1)\n",
    "}\n",
    "\n",
    "# Reducer function\n",
    "word.count.reduce <- function(word, counts) {\n",
    "  keyval(word, sum(counts))\n",
    "}\n",
    "\n",
    "# MapReduce job\n",
    "wordcount <- function(input.path, output.path) {\n",
    "  mapreduce(\n",
    "    input = input.path,\n",
    "    output = output.path,\n",
    "    map = word.count.map,\n",
    "    reduce = word.count.reduce\n",
    "  )\n",
    "}\n",
    "```\n",
    "\n",
    "## 4. PySpark (Advanced Python with Spark)\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "def analyze_sales(spark, input_path):\n",
    "    # Read CSV file\n",
    "    df = spark.read.csv(input_path, header=True)\n",
    "    \n",
    "    # Perform sales analysis\n",
    "    sales_analysis = (\n",
    "        df.groupBy(\"product\")\n",
    "        .agg({\n",
    "            \"quantity\": \"sum\", \n",
    "            \"price\": \"avg\"\n",
    "        })\n",
    "        .orderBy(\"product\")\n",
    "    )\n",
    "    \n",
    "    return sales_analysis\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Sales Analysis\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    result = analyze_sales(spark, \"hdfs://sales/input\")\n",
    "    result.write.csv(\"hdfs://sales/output\")\n",
    "```\n",
    "\n",
    "## 5. Go (Using Hadoop Streaming)\n",
    "```go\n",
    "package main\n",
    "\n",
    "import (\n",
    "    \"bufio\"\n",
    "    \"fmt\"\n",
    "    \"os\"\n",
    "    \"strings\"\n",
    ")\n",
    "\n",
    "func mapper() {\n",
    "    scanner := bufio.NewScanner(os.Stdin)\n",
    "    for scanner.Scan() {\n",
    "        line := scanner.Text()\n",
    "        words := strings.Fields(line)\n",
    "        for _, word := range words {\n",
    "            fmt.Printf(\"%s\\t1\\n\", strings.ToLower(word))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "func reducer() {\n",
    "    scanner := bufio.NewScanner(os.Stdin)\n",
    "    currentWord := \"\"\n",
    "    currentCount := 0\n",
    "\n",
    "    for scanner.Scan() {\n",
    "        parts := strings.Split(scanner.Text(), \"\\t\")\n",
    "        word, count := parts[0], 1\n",
    "\n",
    "        if currentWord == word {\n",
    "            currentCount += count\n",
    "        } else {\n",
    "            if currentWord != \"\" {\n",
    "                fmt.Printf(\"%s\\t%d\\n\", currentWord, currentCount)\n",
    "            }\n",
    "            currentWord = word\n",
    "            currentCount = count\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if currentWord != \"\" {\n",
    "        fmt.Printf(\"%s\\t%d\\n\", currentWord, currentCount)\n",
    "    }\n",
    "}\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop MapReduce"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
